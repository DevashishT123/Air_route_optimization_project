# -*- coding: utf-8 -*-
"""NEW_dataset_and_airport_volume_comparision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vU9jUeRORjZQSJ2RcHZW4t529MjqlWoA
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/vol_data_1405 for Devashish.csv")

df.info()

df['Bkg Branch Code'].nunique()

df.head(30)

df['Bkg Branch Code'].unique()

df['Bkg Branch Code'].nunique()

df['Dlv Branch Code'].unique()

df.info()

df['Dlv Branch Code'].nunique()

df.info()

# prompt: find all the rows where W09-ANGUL BRANCH is bkg branch code and A02-BARODA BRANCH as Dlv branch code

df[(df['Bkg Branch Code'] == 'W09-ANGUL BRANCH') & (df['Dlv Branch Code'] == 'A02-BARODA BRANCH')]

df.drop(['Org Zone', 'Org Region', 'Dlv Zone', 'Dlv Region',     'Series'] , axis=1, inplace=True)

df.head(10)

df.isnull().sum()

# prompt: how many nan rows are there in df, show them

nan_rows = df[df.isnull().any(axis=1)]
print("Number of rows with NaN values:", len(nan_rows))
print("Rows with NaN values:")
nan_rows

# prompt: delete these nan_rows from df

df_cleaned = df.dropna()
print("Number of rows after dropping NaN values:", len(df_cleaned))
df_cleaned.isnull().sum()

import pandas as pd

# Load your data
df = df  # Replace with your actual file path

# Pivot-style aggregation using a groupby
# Filter and aggregate separately, then merge

# Aggregate total weight (for 'foc Billed Wt')
df_weight = (
    df[df['Measure Names'] == 'foc Billed Wt']
    .groupby(['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP'])['Measure Values']
    .sum()
    .reset_index()
    .rename(columns={'Measure Values': 'Total Weight'})
)

# Aggregate total conno (for 'foc_cnno')
df_conno = (
    df[df['Measure Names'] == 'foc_cnno']
    .groupby(['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP'])['Measure Values']
    .sum()
    .reset_index()
    .rename(columns={'Measure Values': 'Total_conno'})
)

# Merge both on common keys
final_df = pd.merge(df_weight, df_conno, on=['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP'], how='outer')

# Fill NaNs with 0 in case some groups had only one type of measure
final_df['Total Weight'] = final_df['Total Weight'].fillna(0)
final_df['Total_conno'] = final_df['Total_conno'].fillna(0)

# Display the result
print(final_df)

final_df.info()

final_df.head(30)

# prompt: an you add the Total_weight and  Total_conno  values for each unique od pair for EP and BP together and Update the NewProduct GCP column name as EP_BP, and for ES write ES and keep the weights as it is in df_final, keep the columns Bkg Branch Code	Dlv Branch Code	New Product GCP	Total Weight	Total_conno

import pandas as pd
# Separate the data for EP/BP and ES
df_ep_bp = final_df[final_df['New Product GCP'].isin(['EP', 'BP'])].copy()
df_es = final_df[final_df['New Product GCP'] == 'ES'].copy()

# For EP and BP, group by OD pair and sum the values
df_ep_bp_agg = df_ep_bp.groupby(['Bkg Branch Code', 'Dlv Branch Code']).agg(
    Total_Weight=('Total Weight', 'sum'),
    Total_conno=('Total_conno', 'sum')
).reset_index()

# Add the new 'New Product GCP' column with the value 'EP_BP'
df_ep_bp_agg['New Product GCP'] = 'EP_BP'

# For ES, keep the data as it is, but ensure the column names match
df_es = df_es.rename(columns={'Total Weight': 'Total_Weight', 'Total_conno': 'Total_conno'})

# Concatenate the two dataframes
df_final_combined = pd.concat([df_ep_bp_agg, df_es], ignore_index=True)

# Select and reorder the desired columns
df_final_combined = df_final_combined[['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP', 'Total_Weight', 'Total_conno']]

# Display the result
print(df_final_combined)
df_final_combined.info()
df_final_combined.head(30)

# prompt: drop the total_conno column and add the Total Weight values for EP and BP rows between unique pair of Bkg Branch Code and Dlv Branch Code and name them EP_BP and keep ES as it is under the same column as New product GCP

import pandas as pd
# Drop the 'Total_conno' column
final_df = final_df.drop('Total_conno', axis=1)

# Group by 'Bkg Branch Code' and 'Dlv Branch Code'
grouped = final_df.groupby(['Bkg Branch Code', 'Dlv Branch Code'])

# Initialize an empty list to store the new rows
new_rows = []

# Iterate through each group (each unique pair of branches)
for name, group in grouped:
    bkg_branch, dlv_branch = name

    # Filter for EP and BP rows within the current group
    ep_bp_rows = group[group['New Product GCP'].isin(['EP', 'BP'])]

    # Sum the 'Total Weight' for EP and BP
    total_weight_ep_bp = ep_bp_rows['Total Weight'].sum()

    # Filter for ES rows within the current group
    es_row = group[group['New Product GCP'] == 'ES']

    # Add the combined EP/BP row
    if total_weight_ep_bp > 0:
        new_rows.append({
            'Bkg Branch Code': bkg_branch,
            'Dlv Branch Code': dlv_branch,
            'New Product GCP': 'EP_BP',
            'Total Weight': total_weight_ep_bp
        })

    # Add the ES row if it exists
    if not es_row.empty:
        new_rows.append({
            'Bkg Branch Code': bkg_branch,
            'Dlv Branch Code': dlv_branch,
            'New Product GCP': 'ES',
            'Total Weight': es_row['Total Weight'].iloc[0]
        })

# Create a new DataFrame from the list of new rows
final_df_transformed = pd.DataFrame(new_rows)

# Display the result
print(final_df_transformed)
final_df_transformed.info()
final_df_transformed.head(30)

final_df_transformed.head(10)

import pandas as pd
from itertools import product

# Load your original dataset
df = final_df_transformed # replace with your actual filename

# Get unique branch codes and product types
bkg_branches = df['Bkg Branch Code'].unique()
dlv_branches = df['Dlv Branch Code'].unique()
gcp_types = ['EP_BP', 'ES']

# Create all possible combinations (Cartesian product)
full_combinations = pd.DataFrame(
    list(product(bkg_branches, dlv_branches, gcp_types)),
    columns=['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP']
)

# Remove same origin and destination pairs
full_combinations = full_combinations[full_combinations['Bkg Branch Code'] != full_combinations['Dlv Branch Code']]

# Merge with the original dataset
merged_df = pd.merge(
    full_combinations,
    df,
    on=['Bkg Branch Code', 'Dlv Branch Code', 'New Product GCP'],
    how='left'
)

# Fill missing Total Weight with 0
merged_df['Total Weight'] = merged_df['Total Weight'].fillna(0)

# Final dataset is ready
print(merged_df.head())

merged_df.info()

# prompt: find all the rows where Total Weight is 0.0, and make it as new dataframe

df_zero_weight = merged_df[merged_df['Total Weight'] == 0.0].copy()

print(df_zero_weight.head())
df_zero_weight.info()

final_df = df_zero_weight

final_df.info()

final_df['Bkg Branch Code'].nunique()

final_df['Dlv Branch Code'].nunique()

final_df.to_csv('Org_des_branch_vol.csv')

# prompt: rename Bkg Branch Code as Origin Branch and Dlv Branch Code as Destination Branch in final_df

final_df = final_df.rename(columns={'Bkg Branch Code': 'Origin Branch', 'Dlv Branch Code': 'Destination Branch'})
print(final_df.head())

final_df.head(20)

"""## Branch - hub - airport - correct mapping final"""

import pandas as pd

# Step 1: Load datasets
df1 = pd.read_csv("/content/branch-hub-airport-mapping(correct).csv")     # Dataset 1: full Branch → Hub → Hub City → Final_Airport_City
df2 = pd.read_csv("/content/Branch-hub-correct-mapping - Sheet1.csv")     # Dataset 2: correct Branch → Refined Hub

# Step 2: Merge to bring in refined hubs
df = df1.merge(df2, on='Branch', how='left')

# Step 3: Replace Hub with Refined Hub (if exists)
df['Hub'] = df['Refined Hub'].combine_first(df['Hub'])
df.drop(columns=['Refined Hub'], inplace=True)

# Step 4: Create a hub lookup dictionary from dataset 1
hub_lookup = df1.drop_duplicates(subset=['Hub'])[['Hub', 'Hub City', 'Final_Airport_City']]

# Step 5: Merge to get updated Hub City and Final_Airport_City
df = df.drop(columns=['Hub City', 'Final_Airport_City'], errors='ignore')
df = df.merge(hub_lookup, on='Hub', how='left')

# Step 6: Finalize column order
df1 = df[['Branch', 'Hub', 'Hub City', 'Final_Airport_City']]

# Step 7: Save to CSV
df1.to_csv("final_updated_branch_hub_airport.csv", index=False)

print("✅ Branch-to-Hub mapping fully updated with correct Hub, Hub City, and Airport City.")
print(df1.head())

df1.head()

# Manual mapping overrides for hub/branch names not directly found
manual_airport_overrides = {
    'B113-ATTIBELE APEX': 'BANGALORE',
    'B36-BANGALORE NELAMANGALA APEX': 'BANGALORE',
    'C20-VELAPPANCHAVADI APEX': 'CHENNAI',
    'C41-GEC CHENNAI': 'CHENNAI',
    'E13-COIMBATORE SCS APEX': 'COIMBATORE',
    'K56-PORT BLAIR BRANCH': 'PORT BLAIR',
    'N41-DELHI SAMHALKHA SURFACE APEX': 'DELHI',
    'R11-NAGPUR PANDE LAYOUT BRANCH': 'NAGPUR',
    'R17-RAIPUR T/S APEX': 'RAIPUR',
    'W10-BHUBANESHWAR SURFACE APEX': 'BHUBANESHWAR',
    'X23-DHUBRI BRANCH': 'GUWAHATI',
    'X35-HGB ROAD APEX': 'AGARTALA',
    'I24-INDORE SURFACE APEX': 'INDORE',
    'R16-NAGPUR SCS APEX': 'NAGPUR',
    'O-': pd.NA  # Unclear mapping, keep as NA
}

final_df.head()

# import pandas as pd

# # Load data
# df_main = final_df  # Your main dataset
# df_map = df1  # Mapping dataset

# # Rename for consistency
# df_map = df_map.rename(columns={
#     'Branch': 'Branch Code',
#     'Hub': 'Hub Code',
#     'Final_Airport_City': 'Airport City'
# })

# # Create two mapping dictionaries: one from Branch, another from Hub
# branch_to_hub = df_map.set_index('Branch Code')['Hub Code'].to_dict()
# branch_to_airport = df_map.set_index('Branch Code')['Airport City'].to_dict()
# hub_to_airport = df_map.set_index('Hub Code')['Airport City'].to_dict()

# # Function to get hub and airport for a branch (or a hub acting as branch)
# def get_hub_airport(branch):
#     # Step 1: Exact match as branch
#     hub = branch_to_hub.get(branch)
#     airport = branch_to_airport.get(branch)

#     # Step 2: Exact match as hub
#     if hub is None and branch in hub_to_airport:
#         hub = branch  # Assume it is a hub
#         airport = hub_to_airport[branch]

#     # Step 3: If still not found, try suffix match (e.g., "-SURFACE APEX")
#     if airport is None or pd.isna(airport):
#         # Extract suffix: everything after the first dash
#         if '-' in branch:
#             name_part = branch.split('-', 1)[1].strip()

#             # Search in Branch Code suffixes
#             branch_match = next(
#                 (k for k in branch_to_airport if k.split('-', 1)[1].strip() == name_part),
#                 None
#             )
#             if branch_match:
#                 hub = branch_to_hub.get(branch_match, branch_match)
#                 airport = branch_to_airport.get(branch_match)

#             # Else search in Hub Code suffixes
#             if airport is None:
#                 hub_match = next(
#                     (k for k in hub_to_airport if k.split('-', 1)[1].strip() == name_part),
#                     None
#                 )
#                 if hub_match:
#                     hub = hub_match
#                     airport = hub_to_airport.get(hub_match)

#     # Fallback
#     if hub is None:
#         hub = branch
#     if airport is None:
#         airport = pd.NA

#     return pd.Series([hub, airport])

# # Apply mapping
# df_main[['Origin Hub', 'Origin Airport']] = df_main['Origin Branch'].apply(get_hub_airport)
# df_main[['Destination Hub', 'Destination Airport']] = df_main['Destination Branch'].apply(get_hub_airport)

# # Final columns order
# final_cols = [
#     'Origin Branch', 'Origin Hub', 'Origin Airport',
#     'Destination Airport', 'Destination Hub', 'Destination Branch',
#     'New Product GCP', 'Total Weight', 'Total_conno'
# ]

# df_final = df_main[final_cols]

# # Preview
# print(df_final.head())

# # Optional: Save output
# # df_final.to_csv('final_with_airport_mapping.csv', index=False)

import pandas as pd

# Load shipment data
df_main = final_df  # Replace with your file

# Load refined branch-hub-airport mapping
df_map = df1

# Rename columns for clarity
df_map = df_map.rename(columns={
    'Branch': 'Branch Code',
    'Hub': 'Hub Code',
    'Final_Airport_City': 'Airport City'
})

# Create mapping dictionaries
branch_to_hub = df_map.set_index('Branch Code')['Hub Code'].to_dict()
branch_to_airport = df_map.set_index('Branch Code')['Airport City'].to_dict()
hub_to_airport = df_map.set_index('Hub Code')['Airport City'].to_dict()

# Function to get hub and airport for a given branch
def get_hub_airport(branch):
    hub = branch_to_hub.get(branch, None)
    airport = branch_to_airport.get(branch, None)

    # If not found in branch, check if it's a hub directly
    if hub is None and branch in hub_to_airport:
        hub = branch
        airport = hub_to_airport[branch]

    # If still not found, try matching by suffix
    if (hub is None or airport is None) and '-' in branch:
        name_part = branch.split('-', 1)[1].strip()

        # Try branch name suffix match
        branch_match = next((k for k in branch_to_hub if k.split('-', 1)[1].strip() == name_part), None)
        if branch_match:
            hub = branch_to_hub.get(branch_match, hub)
            airport = branch_to_airport.get(branch_match, airport)

        # Try hub name suffix match
        if (hub is None or airport is None):
            hub_match = next((k for k in hub_to_airport if k.split('-', 1)[1].strip() == name_part), None)
            if hub_match:
                hub = hub_match
                airport = hub_to_airport.get(hub_match, airport)

    # Final fallback
    if hub is None:
        hub = branch  # fallback to itself
    if airport is None:
        airport = pd.NA

    return pd.Series([hub, airport])

# Apply to Origin and Destination Branches
df_main[['Origin Hub', 'Origin Airport']] = df_main['Origin Branch'].apply(get_hub_airport)
df_main[['Destination Hub', 'Destination Airport']] = df_main['Destination Branch'].apply(get_hub_airport)

# Reorder columns for clarity
final_cols = [
    'Origin Branch', 'Origin Hub', 'Origin Airport',
    'Destination Airport', 'Destination Hub', 'Destination Branch',
    'New Product GCP', 'Total Weight'
]
df_final = df_main[final_cols]

# Preview
print(df_final.head())

# Optional: save to file
# df_final.to_csv('shipment_with_airport_mapping.csv', index=False)

df_final['Origin Airport'].unique()

df_final['Origin Branch'].unique()

df_final['Destination Airport'].unique()

df_final.info()

# prompt: find the corresponding destination hub for which Destination Airport is Null in df_final

# Filter rows where 'Destination Airport' is NaN
df_dest_airport_null = df_final[df_final['Destination Airport'].isnull()]

# Select the 'Destination Hub' for these rows
destination_hubs_for_null_airport = df_dest_airport_null['Destination Hub']

# Print or display the result
print("Destination Hubs where Destination Airport is Null:")
print(destination_hubs_for_null_airport.unique()) # Display unique hubs

# prompt: find the corresponding destination branch for which Destination Airport is Null in df_final

# Filter rows where 'Destination Airport' is NaN
df_dest_airport_null = df_final[df_final['Destination Airport'].isnull()]

# Select the 'Destination Branch' for these rows
destination_branches_for_null_airport = df_dest_airport_null['Destination Branch']

# Print or display the result
print("Destination Branches where Destination Airport is Null:")
print(destination_branches_for_null_airport.unique()) # Display unique branches

# prompt: find the corresponding origin branch for which origin Airport is Null in df_final

# Filter rows where 'Origin Airport' is NaN
df_origin_airport_null = df_final[df_final['Origin Airport'].isnull()]

# Select the 'Origin Branch' for these rows
origin_branches_for_null_airport = df_origin_airport_null['Origin Branch']

# Print or display the result
print("Origin Branches where Origin Airport is Null:")
print(origin_branches_for_null_airport.unique()) # Display unique branches

# prompt: perform manuall mapping of these destination_branches_for_null_airport and origin_branches_for_null_airport to below hub based and the airport mapping on mapping:, make sure df_final also gets updated
# 'A17-BARODA SCS APEX' hub is A01-AHMEDABAD APEX
# 'C20-VELAPPANCHAVADI APEX'  hub is C42-GUINDY APEX
# 'B36-BANGALORE NELAMANGALA APEX' hub is B10-BANGALORE YELAHANKA APEX
# 'R06-RAIPUR APEX' hub is R01-NAGPUR AIR APEX
# 'R11-NAGPUR PANDE LAYOUT BRANCH' hub is R01-NAGPUR AIR APEX
# 'R17-RAIPUR T/S APEX' hub is R01-NAGPUR AIR APEX
# 'X35-HGB ROAD APEX' hub is X03-AGARTALA APEX
# 'X23-DHUBRI BRANCH' hub is X01-DHARAPUR APEX
# 'I24-INDORE SURFACE APEX' hub is I01-INDORE APEX
# 'W10-BHUBANESHWAR SURFACE APEX' hub is W53-BHUBANESWAR AIR APEX
# 'K95-KOLKATA PAKURIA APEX' hub is K16-KOLKATA RAJARHAT APEX
# 'R16-NAGPUR SCS APEX' hub is R01-NAGPUR AIR APEX
# 'C41-GEC CHENNAI' hub is C42-GUINDY APEX
# 'E13-COIMBATORE SCS APEX'  hub is E01-COIMBATORE APEX
# 'M33-MUMBAI VADPE APEX' hub is M10-MUMBAI SAKINAKA APEX
# 'N41-DELHI SAMHALKHA SURFACE APEX' hub is N05-DELHI SAMALKHA APEX

# Manual mapping for branches with null airports to their correct hub and airport
manual_branch_to_hub_airport_mapping = {
    'A17-BARODA SCS APEX': ('A01-AHMEDABAD APEX', 'AHMEDABAD'),
    'C20-VELAPPANCHAVADI APEX': ('C42-GUINDY APEX', 'CHENNAI'),
    'B36-BANGALORE NELAMANGALA APEX': ('B10-BANGALORE YELAHANKA APEX', 'BANGALORE'),
    'R06-RAIPUR APEX': ('R01-NAGPUR AIR APEX', 'NAGPUR'),
    'R11-NAGPUR PANDE LAYOUT BRANCH': ('R01-NAGPUR AIR APEX', 'NAGPUR'),
    'R17-RAIPUR T/S APEX': ('R01-NAGPUR AIR APEX', 'NAGPUR'),
    'X35-HGB ROAD APEX': ('X03-AGARTALA APEX', 'AGARTALA'),
    'X23-DHUBRI BRANCH': ('X01-DHARAPUR APEX', 'GUWAHATI'),
    'I24-INDORE SURFACE APEX': ('I01-INDORE APEX', 'INDORE'),
    'W10-BHUBANESHWAR SURFACE APEX': ('W53-BHUBANESWAR AIR APEX', 'BHUBANESHWAR'),
    'K95-KOLKATA PAKURIA APEX': ('K16-KOLKATA RAJARHAT APEX', 'KOLKATA'),
    'R16-NAGPUR SCS APEX': ('R01-NAGPUR AIR APEX', 'NAGPUR'),
    'C41-GEC CHENNAI': ('C42-GUINDY APEX', 'CHENNAI'),
    'E13-COIMBATORE SCS APEX': ('E01-COIMBATORE APEX', 'COIMBATORE'),
    'M33-MUMBAI VADPE APEX': ('M10-MUMBAI SAKINAKA APEX', 'MUMBAI'),
    'N41-DELHI SAMHALKHA SURFACE APEX': ('N05-DELHI SAMALKHA APEX', 'DELHI'),
}

# Apply manual mapping to update df_final
for branch, (hub, airport) in manual_branch_to_hub_airport_mapping.items():
    # Update Origin side if needed
    df_final.loc[df_final['Origin Branch'] == branch, 'Origin Hub'] = hub
    df_final.loc[df_final['Origin Branch'] == branch, 'Origin Airport'] = airport

    # Update Destination side if needed
    df_final.loc[df_final['Destination Branch'] == branch, 'Destination Hub'] = hub
    df_final.loc[df_final['Destination Branch'] == branch, 'Destination Airport'] = airport

print("\nDataFrame df_final updated with manual mappings:")
print(df_final.head())

# Verify the updates for some branches that had null airports
print("\nVerification after manual mapping:")
print(df_final[df_final['Origin Branch'].isin(manual_branch_to_hub_airport_mapping.keys())].head())
print(df_final[df_final['Destination Branch'].isin(manual_branch_to_hub_airport_mapping.keys())].head())

# Check if there are still null airports after manual mapping
print("\nRemaining null Origin Airports after manual mapping:")
print(df_final[df_final['Origin Airport'].isnull()]['Origin Branch'].unique())

print("\nRemaining null Destination Airports after manual mapping:")
print(df_final[df_final['Destination Airport'].isnull()]['Destination Branch'].unique())

df_final.info()

# prompt: remove the null corresponding values

df_final_cleaned = df_final.dropna(subset=['Origin Airport', 'Destination Airport'])

print("\nDataFrame after removing rows with null Origin or Destination Airport:")
print(df_final_cleaned.head())

print("\nRemaining null Origin Airports after removal:")
print(df_final_cleaned['Origin Airport'].isnull().sum())

print("\nRemaining null Destination Airports after removal:")
print(df_final_cleaned['Destination Airport'].isnull().sum())

df_final_cleaned.info()

df_final_cleaned.head(20)

df_final = df_final_cleaned

df_final.head()

# prompt: divide the Total Weight and Total_conno values for each by 25 in df_final

df_final['Total Weight'] = df_final['Total Weight'] / 25.0

print(df_final.head())

df_final.head(20)

df_final.info()

df_final.to_csv('Daily_data_origin_to_dest_branch.csv')

# prompt: can you add the Total_weight and  Total_conno  values for each unique od pair for EP and BP together and Update the NewProduct GCP column name as EP_BP, and for ES write ES and keep the weights as it is in df_final, keep the columns Origin Branch, Origin Hub    Destination Hub  Destination BranchOrigin Airport and  Destination Airport also

import pandas as pd
# Define the key columns for the OD pair
od_cols = ['Origin Branch', 'Origin Hub', 'Origin Airport',
           'Destination Airport', 'Destination Hub', 'Destination Branch']

# Create a unique OD pair identifier
df_final['OD_Pair'] = df_final[od_cols].agg('_'.join, axis=1)

# Separate EP/BP and ES data
df_ep_bp = df_final[df_final['New Product GCP'].isin(['EP', 'BP'])].copy()
df_es = df_final[df_final['New Product GCP'] == 'ES'].copy()

# Group EP/BP data by OD Pair and sum the weight and conno
df_ep_bp_agg = df_ep_bp.groupby('OD_Pair')[['Total Weight', 'Total_conno']].sum().reset_index()

# Merge the aggregated EP/BP data back with the original (to get the other columns)
# We need to be careful here to keep only one row per aggregated OD pair.
# We can merge and then drop duplicates based on the OD_Pair, picking one set of branch/hub/airport columns.
# Since the branch/hub/airport columns are the same for all rows within the same OD_Pair by definition of the OD_Pair.
df_ep_bp_merged = pd.merge(
    df_ep_bp_agg,
    df_ep_bp.drop(columns=['Total Weight', 'Total_conno', 'New Product GCP']), # Drop original measure columns and GCP
    on='OD_Pair',
    how='left'
).drop_duplicates(subset=['OD_Pair']) # Keep only one row per OD_Pair

# Add the new 'EP_BP' column
df_ep_bp_merged['New Product GCP'] = 'EP_BP'

# Add the 'EP_BP' column to the ES data
df_es['New Product GCP'] = 'ES'

# Concatenate the aggregated EP/BP data and the ES data
df_combined = pd.concat([df_ep_bp_merged, df_es], ignore_index=True)

# Drop the temporary OD_Pair column
df_combined = df_combined.drop(columns=['OD_Pair'])

# Reorder columns to match the desired output structure
final_combined_cols = [
    'Origin Branch', 'Origin Hub', 'Origin Airport',
    'Destination Airport', 'Destination Hub', 'Destination Branch',
    'New Product GCP', 'Total Weight', 'Total_conno'
]

df_final_updated = df_combined[final_combined_cols]

# Display the result
print(df_final_updated.head())
print(df_final_updated['New Product GCP'].unique())
df_final_updated.info()
df_final_updated.head(20)

df_final['New Product GCP'].unique()

df_final.to_csv('Daily_datafor_(EP+BP)_ES_updated_new_rows.csv')

df_final.info()

"""##Updated dataset generation"""

import pandas as pd
import numpy as np

df_final_updated = pd.read_csv('Daily_datafor_(EP+BP)_ES_updated_new_rows.csv')

df_final_updated.info()

df_final_updated = df_final_updated.drop('Unnamed: 0', axis = 1)

import pandas as pd

# Load your flight connections dataset
flight_df = pd.read_csv("/content/flight_connection.csv")  # Contains: Aeroplane category, origin_city, destination_city

# Normalize casing
flight_df['origin_city'] = flight_df['origin_city'].str.upper().str.strip()
flight_df['destination_city'] = flight_df['destination_city'].str.upper().str.strip()
flight_df['Aeroplane category'] = flight_df['Aeroplane category'].str.upper().str.strip()

# Build a lookup for flights
from collections import defaultdict

# A dictionary of (origin, destination) → list of categories
flight_map = defaultdict(set)
for _, row in flight_df.iterrows():
    key = (row['origin_city'], row['destination_city'])
    flight_map[key].add(row['Aeroplane category'])

# Function to determine flight mode
def determine_mode(row):
    origin = str(row['Origin Airport']).strip().upper()
    dest = str(row['Destination Airport']).strip().upper()
    gcp = row['New Product GCP']

    key = (origin, dest)
    available = flight_map.get(key, set())

    has_prime = 'PRIME' in available
    has_gcr = 'GCR' in available

    if has_prime and has_gcr:
        return 'PRIME' if gcp == 'EP_BP' else 'GCR'
    elif has_gcr:
        return 'GCR'
    elif has_prime:
        return 'PRIME' if gcp == 'EP_BP' else 'Road'
    else:
        return 'Road'

# Apply it
df_final_updated['Flight Mode'] = df_final_updated.apply(determine_mode, axis=1)

# Preview
print(df_final_updated[['Origin Airport', 'Destination Airport', 'New Product GCP', 'Flight Mode']].head())

# Optional: save result
# final_df.to_csv("final_with_flight_mode.csv", index=False)

df_final_updated.head(20)

df_final_updated['Flight Mode'].unique()

import pandas as pd

# Load your shipment dataset
shipment_df = df_final_updated

# Load airport-to-airport volume flow
airport_flow_df = pd.read_csv('/content/Total_volume_from_airport_to_air(corrected) (1).csv')

# Create a lookup dictionary for quick access to volume flow
airport_volume_dict = {(row['Origin_Airport'], row['Destination_Airport']): row['Volume_kg']
                       for _, row in airport_flow_df.iterrows()}

# Function to determine if alternate path is required
def check_path_required(row):
    mode = row['Flight Mode']
    product = row['New Product GCP']
    origin_airport = row['Origin Airport']
    destination_airport = row['Destination Airport']

    # Rule 1: If mode is 'Road', always YES
    if mode.upper() == 'ROAD':
        return 'YES'

    # Get the airport-to-airport volume (if exists, else 0)
    volume = airport_volume_dict.get((origin_airport, destination_airport), 0)

    # Rule 2: EP_BP logic
    if product == 'EP_BP':
        if mode.upper() in ['PRIME', 'GCR']:
            return 'NO' if volume >= 100 else 'YES'
        else:
            return 'YES'

    # Rule 3: ES logic
    elif product == 'ES':
        if mode.upper() == 'GCR':
            return 'NO' if volume >= 100 else 'YES'
        elif mode.upper() == 'PRIME':
            return 'YES'
        else:
            return 'YES'

    # Default fallback
    return 'YES'

# Apply the logic to each row
shipment_df['Path_Required'] = shipment_df.apply(check_path_required, axis=1)

# Optional: Save output
# shipment_df.to_csv('shipment_with_path_required.csv', index=False)

# Display sample output
print(shipment_df[['Origin Branch', 'Destination Branch', 'Flight Mode', 'New Product GCP', 'Origin Airport', 'Destination Airport', 'Path_Required']].head(10))

shipment_df.head()

import pandas as pd

# Load your updated dataset
df = df_final_updated  # replace with actual path

# Define decision logic
def determine_path_required(row):
    product = row['New Product GCP']
    mode = row['Flight Mode']
    weight = row['Total Weight']

    # Rule 1: If by Road → Always requires alternate path
    if mode == 'Road':
        return 'YES'

    # Rule 2: EP_BP logic
    if product == 'EP_BP':
        if mode == 'PRIME':
            return 'NO' if weight >= 100 else 'YES'
        elif mode == 'GCR':
            return 'YES'
        else:
            return 'YES'

    # Rule 3: ES logic
    elif product == 'ES':
        if mode == 'GCR':
            return 'NO' if weight >= 100 else 'YES'
        elif mode == 'PRIME':
            return 'YES'
        else:
            return 'YES'

    # Default fallback
    return 'YES'

# Apply logic
df['Path_Required'] = df.apply(determine_path_required, axis=1)

# Save the updated file
# df.to_csv("shipment_data_with_path_flag.csv", index=False)

# Preview
print(df[['Origin Branch', 'Destination Branch', 'New Product GCP', 'Total Weight', 'Flight Mode', 'Path_Required']].head())

# prompt: number of rows which are No in path_required column

# Count rows where 'Path_Required' is 'NO'
count_no_path_required = df[df['Path_Required'] == 'NO'].shape[0]

print(f"Number of rows where Path_Required is 'NO': {count_no_path_required}")

df.info()

df.to_csv('/content/Daily_datafor_(EP+BP)_ES_path_required_(with_new_rows).csv')

"""## Airport Volume to Volume Comparision(ACTUAL VS CALC)"""

import pandas as pd

# Load your latest dataset
df = pd.read_csv('/content/airport_to_airport_volume_by_product.csv')  # use your actual file name

# Group by Origin Airport, Destination Airport, and Product Type, and sum Total Weight
agg_df = df.groupby(
    ['Origin_Airport', 'Destination_Airport'],
    as_index=False
)['Volume_kg'].sum()

# Optional: Rename columns for clarity
agg_df.rename(columns={
    'Origin_Airport': 'Origin_Airport',
    'Destination_Airport': 'Destination_Airport',
    'Volume_kg': 'Volume_kg'
}, inplace=True)

# Save to CSV
# agg_df.to_csv("airport_to_airport_volume_by_product.csv", index=False)

# Preview
print(agg_df.head())

agg_df.to_csv('airport_to_airport_volume(calculated).csv')

df.head()

df1 = df.copy()

df1.head()

df1.drop(columns=['Unnamed: 0'], inplace=True)

import pandas as pd

# Load your shipment dataset
df = df1# replace with your actual filename

# Filter only rows where flight mode is not 'Road'
flight_df = df[df['Flight Mode'].str.upper() != 'ROAD']

# Group by Origin Airport, Destination Airport, and Product Type
agg_flight_volume = flight_df.groupby(
    ['Origin Airport', 'Destination Airport'],
    as_index=False
)['Total Weight'].sum()

# Rename columns for clarity
agg_flight_volume.rename(columns={
    'Origin Airport': 'Origin_Airport',
    'Destination Airport': 'Destination_Airport',
    'Total Weight': 'Volume_kg'
}, inplace=True)

# Save to CSV
agg_flight_volume.to_csv("airport_to_airport_direct_flight_volume.csv", index=False)

# Optional: Preview
print(agg_flight_volume.head())

agg_flight_volume.info()

agg_flight_volume['Origin_Airport'].unique()

df2 = pd.read_csv('/content/Airport_to_airport_volume(Actual).csv')

df2['origin_city'].unique()

df2.head(20)

df2.drop('Unnamed: 0', axis = 1)

agg_flight_volume.head(20)

import pandas as pd

# Load the datasets
actual_df = df2  # contains origin_city, destination_city, Actual Volume
calculated_df = agg_flight_volume  # contains Origin_Airport, Destination_Airport, Volume_kg

# Standardize column names
actual_df.rename(columns={
    'origin_city': 'Origin_Airport',
    'destination_city': 'Destination_Airport',
    'Actual Volume': 'Actual_Volume_kg'
}, inplace=True)

# Merge both datasets on OD pairs
merged = pd.merge(
    actual_df,
    calculated_df,
    on=['Origin_Airport', 'Destination_Airport'],
    how='outer'
)

# Fill missing volumes with zero
merged['Actual_Volume_kg'] = merged['Actual_Volume_kg'].fillna(0)
merged['Volume_kg'] = merged['Volume_kg'].fillna(0)

# Calculate difference and percentage gap
merged['Volume_Diff_kg'] = merged['Actual_Volume_kg'] - merged['Volume_kg']
merged['Percentage_Diff'] = merged.apply(
    lambda row: 0 if row['Actual_Volume_kg'] == 0 else (row['Volume_Diff_kg'] / row['Actual_Volume_kg']) * 100,
    axis=1
)

# Add flags for insights
merged['Flag'] = merged.apply(lambda row:
    'Missing in Calculated' if row['Volume_kg'] == 0 and row['Actual_Volume_kg'] > 0 else
    'Missing in Actual' if row['Actual_Volume_kg'] == 0 and row['Volume_kg'] > 0 else
    'Significant Gap' if abs(row['Percentage_Diff']) > 30 else
    'Match',
    axis=1
)

# Save to file
merged.to_csv("airport_volume_comparison.csv", index=False)

# Preview top insights
insights = merged.sort_values(by='Percentage_Diff', key=abs, ascending=False).head(20)
print(insights[['Origin_Airport', 'Destination_Airport', 'Actual_Volume_kg', 'Volume_kg', 'Volume_Diff_kg', 'Percentage_Diff', 'Flag']])

insights.info()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the merged comparison file
df = pd.read_csv("airport_volume_comparison.csv")

# Set plot style
sns.set(style="whitegrid")

# -------- 1. Top Absolute Differences (Bar Chart) --------
top_abs_diff = df.copy()
top_abs_diff['Abs_Diff'] = top_abs_diff['Volume_Diff_kg'].abs()
top_abs_diff = top_abs_diff.sort_values(by='Abs_Diff', ascending=False).head(15)

plt.figure(figsize=(12, 6))
sns.barplot(
    data=top_abs_diff,
    x='Abs_Diff',
    y=top_abs_diff['Origin_Airport'] + " → " + top_abs_diff['Destination_Airport'],
    palette='coolwarm'
)
plt.title("Top 15 Airport OD Pairs by Absolute Volume Gap")
plt.xlabel("Absolute Volume Difference (kg)")
plt.ylabel("OD Pair")
plt.tight_layout()
plt.show()

# -------- 2. Percentage Gap Bar Plot (Filtered) --------
filtered = df[(df['Actual_Volume_kg'] > 50) & (df['Volume_kg'] > 0)]
top_pct_diff = filtered.sort_values(by='Percentage_Diff', key=abs, ascending=False).head(15)

plt.figure(figsize=(12, 6))
sns.barplot(
    data=top_pct_diff,
    x='Percentage_Diff',
    y=top_pct_diff['Origin_Airport'] + " → " + top_pct_diff['Destination_Airport'],
    palette='vlag'
)
plt.title("Top 15 Airport OD Pairs by % Volume Difference")
plt.xlabel("Percentage Gap (%)")
plt.ylabel("OD Pair")
plt.axvline(0, color='black', linestyle='--')
plt.tight_layout()
plt.show()

# -------- 3. Heatmap (Optional, for Dense Data) --------
pivot = df.pivot_table(
    index='Origin_Airport',
    columns='Destination_Airport',
    values='Volume_Diff_kg',
    fill_value=0
)

plt.figure(figsize=(14, 10))
sns.heatmap(pivot, cmap='RdBu_r', center=0, annot=False)
plt.title("Airport-to-Airport Volume Gap (Actual - Calculated)")
plt.xlabel("Destination Airport")
plt.ylabel("Origin Airport")
plt.tight_layout()
plt.show()

"""## Indirect route finding"""

df.head()

import pandas as pd

# Load your final dataset
df = df

# Filter rows where alternate path is required
df_alt_required = df[df['Path_Required'] == "YES"].copy()

df.head(10)

df.head()

import pandas as pd

# Load your full dataset
df = df # Replace with your actual file path
df_filtered = df[df['Origin Airport'] != df['Destination Airport']]

# Group by Origin Airport and Destination Airport and sum the Total Weight
airport_flow = (
    df_filtered.groupby(['Origin Airport', 'Destination Airport'], as_index=False)
      .agg({'Total Weight': 'sum'})
      .rename(columns={'Total Weight': 'Total Volume (kg)'})
      .sort_values(by='Total Volume (kg)', ascending=False)
)

# Preview result
print(airport_flow.head())

# Optional: Save to CSV
# airport_flow.to_csv('airport_to_airport_volume.csv', index=False)

airport_flow.info()

airport_flow.head(10)

airport_flow.to_csv('Total_volume_from_airport_to_air.csv')

"""## Reading Branch master"""

df_branch = pd.read_excel('/content/Branch Master.xlsx')

df_branch.head()

# prompt: merge Branch and Branch_Name column under one column Branch_Name in manner like A01-AHMEDABAD APEX for df_branch

df_branch['Branch_Name'] = df_branch['Branch'] + '-' + df_branch['Branch_Name']
df_branch.head()

# prompt: drop column Branch from df_branch

df_branch.drop('Branch', axis=1, inplace=True)
df_branch.head()

# prompt: find out all the rows where HUB_CODE = H

df_hub_h = df_branch[df_branch['HUB_CODE'] == 'H'].copy()
df_hub_h

df_hub_h.info()

# prompt: take two columns Branch_Name and Branch City	from df_hub_h and asign new dataframe

df_selected = df_hub_h[['Branch_Name', 'Branch City']]
print(df_selected.head())
df_selected.info()

df_selected.to_csv('all_branch_hub-mapping.csv')

import pandas as pd

# Load hub to city mapping
hub_df =  df_selected # columns: Branch_Name, Branch City

# List of explicitly known Air Hubs (from your input)
explicit_air_hubs = [
    "A04-SURAT APEX", "A01-AHMEDABAD APEX", "V02-VISHAKAPATNAM APEX", "V06-VIJAYAWADA APEX",
    "E04-SALEM APEX", "E01-COIMBATORE APEX", "I01-INDORE APEX", "U43-LUCKNOW APEX",
    "M10-MUMBAI SAKINAKA APEX", "Q05-JAIPUR APEX", "R01-NAGPUR APEX",
    "N05-DELHI SAMALKHA APEX", "J29-PANCHKULA APEX", "C42-GUINDY APEX",
    "B10-BANGALORE YELAHANKA APEX", "W53-BHUBANESWAR AIR APEX"
]

# Assign hub types
hub_df["Hub_Type"] = hub_df["Branch_Name"].apply(
    lambda x: "Air Hub" if x in explicit_air_hubs else "Surface Hub"
)

# Fix: For cities with only one hub, assign it as "Air Hub"
city_counts = hub_df["Branch City"].value_counts()
single_hub_cities = city_counts[city_counts == 1].index.tolist()

hub_df.loc[hub_df["Branch City"].isin(single_hub_cities), "Hub_Type"] = "Air Hub"

# Output
hub_df.to_csv("hub_city_with_types.csv", index=False)

hub_df.head(10)

df = pd.DataFrame(hub_df)

df.head()

df['Hub_Type'].unique()

import pandas as pd

# Load hub to city mapping
hub_df = df_selected # columns: Branch_Name, Branch City

# Step 1: Mark explicitly known air hubs
explicit_air_hubs = [
    "A04-SURAT APEX", "A01-AHMEDABAD APEX", "V02-VISHAKAPATNAM APEX", "V06-VIJAYAWADA APEX",
    "E04-SALEM APEX", "E01-COIMBATORE APEX", "I01-INDORE APEX", "U43-LUCKNOW APEX",
    "M10-MUMBAI SAKINAKA APEX", "Q05-JAIPUR APEX", "R01-NAGPUR APEX", "R06-RAIPUR APEX",
    "N05-DELHI SAMALKHA APEX", "J29-PANCHKULA APEX", "C42-GUINDY APEX",
    "B10-BANGALORE YELAHANKA APEX", "W53-BHUBANESWAR AIR APEX"
]

# Step 2: Count how many hubs exist per city
city_counts = hub_df["Branch City"].value_counts()

# Step 3: Assign Hub_Type
def classify_hub(row):
    city = row["Branch City"]
    hub = row["Branch_Name"]

    if city_counts[city] == 1:
        return "Air & Surface Hub"
    elif hub in explicit_air_hubs:
        return "Air Hub"
    else:
        return "Surface Hub"

hub_df["Hub_Type"] = hub_df.apply(classify_hub, axis=1)

# Output the result
hub_df.to_csv("hub_city_with_hub_type.csv", index=False)

hub_df.head()

hub_df.to_csv('Hub_(Surface & Air).csv')

"""# Step 1: Load All Input Files into DataFrames"""

import pandas as pd

# Final filtered dataset where Path_Required == 'YES'
df = pd.read_csv("/content/Daily_datafor_(EP+BP)_ES_path_required_updated.csv")  # Replace with actual file

# Time matrices
branch_to_hub_time = pd.read_csv("/content/branch_to_hub_time_haversine.csv")
hub_to_airport_time = pd.read_csv("/content/Hub_to_airport_time_matrix - Sheet1.csv")
airport_to_airport_time = pd.read_csv("/content/acutal_flight_time_matrix.csv")
hub_to_hub_time = pd.read_csv("/content/hub_to_hub_time.csv")

# Clean up column names
airport_to_airport_time.rename(columns={
    'Origin': 'Origin_Airport',
    'Destination': 'Destination_Airport',
    'Estimated_Flight_Time_Minutes': 'Flight_Time_Minutes'
}, inplace=True)

df.head()

"""# Step 2: Load Airport Volume Flow (for checking ≥ 45 kg viability)"""

airport_flow.head()

# Volume between Origin and Middle Airport
airport_volume_df = pd.read_csv("/content/Total_volume_from_airport_to_air.csv")  # Replace with actual file
airport_volume_df.rename(columns={
    'Origin Airport': 'Origin_Airport',
    'Destination Airport': 'Destination_Airport',
    'Total Volume (kg)': 'Volume_kg'
}, inplace=True)

airport_volume_df.head()

airport_volume_df = airport_volume_df.drop('Unnamed: 0', axis = 1)

airport_volume_df.to_csv('Total_volume_from_airport_to_air(sample).csv')

airport_flow.head()

"""# Step 3: Load Flight Availability Between Airport Pairs"""

flight_df.head()



flight_df = pd.read_csv("/content/flight_connection.csv")  # Should include origin, destination, type (PRIME/GCR)
flight_df.rename(columns={
    'origin_city': 'Origin City',
    'destination_city': 'Destination City',
    'Aeroplane category': 'Flight Type'
}, inplace=True)

flight_df = flight_df.drop('Unnamed: 0', axis = 1)

flight_df.to_csv('flight_connection(sample).csv')

"""# Step 4: Identify Viable Middle Airports"""

df = pd.read_csv('/content/Daily_datafor_(EP+BP)_ES_path_required_updated.csv')

import pandas as pd

# Step 1: Filter only rows needing alternate path

df_filtered = df[df['Path_Required'] == "YES"].copy()

# Step 2: Merge with flight data to get all valid mid airports
flight_paths = flight_df[['Origin_Airport', 'Destination_Airport']].copy()
flight_paths = flight_paths.rename(columns={'Destination_Airport': 'Middle_Airport'})

# Step 3: Merge with airport-to-airport volumes
volume_df = airport_volume_df.rename(columns={
    'Origin_Airport': 'Origin_Airport',
    'Destination_Airport': 'Middle_Airport',
    'Volume_kg': 'Existing_Volume'
})

flight_volume = pd.merge(flight_paths, volume_df, on=['Origin_Airport', 'Middle_Airport'], how='left')
flight_volume['Existing_Volume'] = flight_volume['Existing_Volume'].fillna(0)

# Step 4: Explode df_filtered by joining it with all its possible middle airports
df_joined = pd.merge(
    df_filtered[['Origin Airport', 'Destination Branch', 'Total Weight', 'New Product GCP']],
    flight_volume,
    left_on='Origin Airport',
    right_on='Origin_Airport',
    how='left'
)

# Step 5: Compute Total Volume after adding shipment
df_joined['Total_Volume'] = df_joined['Existing_Volume'] + df_joined['Total Weight']

# Step 6: Filter only rows where total volume ≥ 45
df_viable = df_joined[df_joined['Total_Volume'] >= 45].copy()

# Step 7: Group by each (Origin, Destination Branch) and get top 3 by Total Volume
df_viable['key'] = df_viable['Origin_Airport'] + "_" + df_viable['Destination Branch']
top_middle_df = (
    df_viable.sort_values(['key', 'Total_Volume'], ascending=[True, False])
    .groupby('key')
    .head(6)
)

# Step 8: Aggregate back to original format with a list of viable airports and volumes
final_middle_map = (
    top_middle_df[['Middle_Airport', 'Total_Volume', 'key']]
    .groupby('key')
    .apply(lambda x: list(zip(x['Middle_Airport'], x['Total_Volume'].round(2))))
    .reset_index()
    .rename(columns={0: 'Viable_Middle_Airports'})
)

# Step 9: Merge back to original filtered df
df_filtered['key'] = df_filtered['Origin Airport'] + "_" + df_filtered['Destination Branch']
df_final = pd.merge(df_filtered, final_middle_map, on='key', how='left').drop(columns='key')

df_final.head(20)

df_final = df_final.drop('Unnamed: 0', axis = 1)

df_final['Viable_Middle_Airports'][0]

df_final.head(10)

airport_volume.head()

# airport_volume = airport_volume.drop('Unnamed: 0', axis = 1)
airport_volume.columns = ['Origin_Airport', 'Middle_Airport', 'Volume_kg']

flight_time.head()

flight_time = flight_time.drop('Unnamed: 0', axis = 1)
flight_time.columns = ['Origin', 'Destination', 'FlightTime_Min']

flight_connections.head()

flight_connections = flight_connections.drop('Unnamed: 0', axis = 1)
flight_connections.columns = ['Origin', 'Destination', 'Mode']

hub_classification.head()

hub_classification = hub_classification.drop('Unnamed: 0', axis = 1)
hub_classification.columns = ['Hub', 'City', 'Hub_Type']

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional

class LogisticsRouteOptimizer:
    """Optimized logistics route analysis with improved performance and maintainability."""

    def __init__(self):
        self.lookups = {}
        self.hub_info = {}
        self.middle_airports = []

    def load_and_preprocess_data(self) -> pd.DataFrame:
        """Load and preprocess all datasets with optimized operations."""
        # Load datasets
        shipments = pd.read_csv("/content/Daily_datafor_(EP+BP)_ES_path_required_updated.csv")
        branch_to_hub = pd.read_csv("/content/branch_to_hub_time_haversine.csv")
        hub_to_airport = pd.read_csv("/content/Hub_to_airport_time_matrix - Sheet1.csv")
        hub_to_hub = pd.read_csv("/content/hub_to_hub_time.csv")
        airport_volume = pd.read_csv("/content/Total_volume_from_airport_to_air.csv")
        flight_time = pd.read_csv("/content/acutal_flight_time_matrix.csv")
        flight_connections = pd.read_csv("/content/flight_connection.csv")
        hub_classification = pd.read_csv("/content/Hub_(Surface & Air).csv")

        # Standardize column names by dropping 'Unnamed: 0' first where it exists
        if 'Unnamed: 0' in shipments.columns:
            shipments.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in airport_volume.columns:
            airport_volume.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in flight_time.columns:
            flight_time.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in flight_connections.columns:
            flight_connections.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in hub_classification.columns:
            hub_classification.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in branch_to_hub.columns:
            branch_to_hub.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in hub_to_airport.columns:
            hub_to_airport.drop('Unnamed: 0', axis=1, inplace=True)
        if 'Unnamed: 0' in hub_to_hub.columns:
            hub_to_hub.drop('Unnamed: 0', axis=1, inplace=True)


        # Rename columns
        airport_volume.columns = ['Origin_Airport', 'Middle_Airport', 'Volume_kg']
        flight_time.columns = ['Origin', 'Destination', 'FlightTime_Min']
        flight_connections.columns = ['Origin', 'Destination', 'Mode']
        hub_classification.columns = ['Hub', 'City', 'Hub_Type']
        branch_to_hub.columns = ['Branch', 'Hub', 'Distance_km', 'Duration_hr']
        hub_to_airport.columns = ['Hub', 'Final_Airport_City', 'Distance_km', 'Duration_hr']
        hub_to_hub.columns = ['Hub1', 'Hub2', 'Distance_km', 'Duration_hr']


        # Preprocess flight time data
        flight_time['FlightTime_Hr'] = flight_time['FlightTime_Min'] / 60

        # Create optimized lookup dictionaries
        self._create_lookup_tables(branch_to_hub, hub_to_airport, hub_to_hub, airport_volume, flight_time, flight_connections, hub_classification)

        # Filter and prepare main dataset
        df = self._filter_shipments(shipments)

        return df

    def _create_lookup_tables(self, branch_to_hub: pd.DataFrame,
                            hub_to_airport: pd.DataFrame,
                            hub_to_hub: pd.DataFrame,
                            airport_volume: pd.DataFrame,
                            flight_time: pd.DataFrame,
                            flight_connections: pd.DataFrame,
                            hub_classification: pd.DataFrame) -> None:
        """Create optimized lookup dictionaries for faster access."""

        # Time-based lookups
        self.lookups['branch_hub'] = dict(
            zip(zip(branch_to_hub['Branch'], branch_to_hub['Hub']),
                branch_to_hub['Duration_hr'])
        )

        self.lookups['hub_airport'] = dict(
            zip(zip(hub_to_airport['Hub'], hub_to_airport['Final_Airport_City']),
                hub_to_airport['Duration_hr'])
        )

        self.lookups['hub_to_hub'] = dict(
            zip(zip(hub_to_hub['Hub1'], hub_to_hub['Hub2']),
                hub_to_hub['Duration_hr'])
        )

        # Flight-related lookups
        self.lookups['airport_flight'] = dict(
            zip(zip(flight_time['Origin'], flight_time['Destination']),
                flight_time['FlightTime_Hr'])
        )

        # Pre-compute flight connections set for O(1) lookup
        self.lookups['flight_connections'] = set(
            zip(flight_connections['Origin'], flight_connections['Destination'])
        )

        # Volume lookup
        self.lookups['volume'] = dict(
            zip(zip(airport_volume['Origin_Airport'], airport_volume['Middle_Airport']),
                airport_volume['Volume_kg'])
        )
        self.middle_airports = sorted(set(airport_volume['Middle_Airport']))

        # Hub classification with optimized structure
        self._create_hub_mappings(hub_classification)

    def _create_hub_mappings(self, hub_classification: pd.DataFrame) -> None:
        """Create optimized hub mappings for faster lookups."""
        # Group hubs by city and type for O(1) access
        self.hub_info['city_to_air_hub'] = {}
        self.hub_info['city_to_surface_hub'] = {}

        for _, row in hub_classification.iterrows():
            city, hub, hub_type = row['City'], row['Hub'], row['Hub_Type']

            if hub_type == 'Air Hub':
                self.hub_info['city_to_air_hub'][city] = hub
            elif hub_type == 'Surface Hub':
                self.hub_info['city_to_surface_hub'][city] = hub
            elif hub_type == 'Air & Surface Hub':
                self.hub_info['city_to_air_hub'][city] = hub
                self.hub_info['city_to_surface_hub'][city] = hub

    def _filter_shipments(self, shipments: pd.DataFrame) -> pd.DataFrame:
        """Filter shipments with vectorized operations."""
        return shipments[
            (shipments['Path_Required'] == 'YES') &
            (shipments['Origin Branch'] != shipments['Destination Branch'])
        ].copy()

    def calculate_route_time(self, row: pd.Series, middle_airport: str) -> Optional[float]:
        """Calculate total route time with optimized lookups."""

        # Extract route information
        origin_branch = row['Origin Branch']
        origin_hub = row['Origin Hub']
        origin_airport = row['Origin Airport']
        dest_branch = row['Destination Branch']
        dest_hub = row['Destination Hub']
        dest_airport = row['Destination Airport']
        product = row['New Product GCP']

        # Get middle airport hubs
        middle_air_hub = self.hub_info['city_to_air_hub'].get(middle_airport)
        if not middle_air_hub:
            return None  # No air hub, skip

        middle_surface_hub = self.hub_info['city_to_surface_hub'].get(middle_airport)

        # Calculate step times using optimized lookups
        t1 = self.lookups['branch_hub'].get((origin_branch, origin_hub), 0)
        t2 = self.lookups['hub_airport'].get((origin_hub, origin_airport), 0)
        t3 = self.lookups['airport_flight'].get((origin_airport, middle_airport), np.inf)
        t4 = self.lookups['hub_airport'].get((middle_air_hub, middle_airport), 0)

        # Handle surface hub time only if a surface hub exists for the middle airport
        t5 = self.lookups['hub_to_hub'].get((middle_air_hub, middle_surface_hub), 0) if middle_surface_hub else 0
        t6 = self.lookups['hub_to_hub'].get((middle_surface_hub if middle_surface_hub else middle_air_hub, dest_hub), 0)
        t7 = self.lookups['branch_hub'].get((dest_branch, dest_hub), 0)

        # Processing and cooling times
        processing_time = 2.5 * (1 + int(bool(middle_surface_hub)))  # 2.5 hrs at each hub
        cooling_time = 4  # 2 in-cooling + 2 out-cooling

        total_time = t1 + t2 + t3 + t4 + t5 + t6 + t7 + processing_time + cooling_time

        return round(total_time, 2) if total_time != np.inf else None

    def find_viable_routes(self, row: pd.Series) -> List[Tuple[str, float, float]]:
        """Find viable routes for a shipment with optimized filtering."""
        origin_airport = row['Origin Airport']
        current_volume = row['Total Weight']
        viable_paths = []

        for middle_airport in self.middle_airports:
            # Check flight connectivity first (fastest filter)
            if (origin_airport, middle_airport) not in self.lookups['flight_connections']:
                continue

            # Check volume constraint
            vol = self.lookups['volume'].get((origin_airport, middle_airport), 0)
            total_vol = vol + current_volume

            if total_vol >= 35:  # Volume threshold
                total_time = self.calculate_route_time(row, middle_airport)

                if total_time and total_time < 100:  # Time constraint
                    viable_paths.append((middle_airport, round(total_vol, 2), total_time))

        # Return top 3 routes sorted by time
        return sorted(viable_paths, key=lambda x: x[2])[:3]

    def process_all_shipments(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process all shipments with optimized batch operations."""
        print(f"Processing {len(df)} shipments...")

        # Use list comprehension for better performance
        viable_routes = [self.find_viable_routes(row) for _, row in df.iterrows()]

        df['Viable_Middle_Airports'] = viable_routes
        return df

    def run_optimization(self, output_path: str = "final_step4_step5_output.csv") -> pd.DataFrame:
        """Main execution method with full optimization pipeline."""

        # Load and preprocess data
        df = self.load_and_preprocess_data()

        # Process all shipments
        df = self.process_all_shipments(df)

        # Save results
        df.to_csv(output_path, index=False)
        print(f"Results saved to {output_path}")

        # Display sample results
        sample_columns = [
            'Origin Branch', 'Destination Branch', 'New Product GCP',
            'Origin Airport', 'Destination Airport', 'Viable_Middle_Airports'
        ]

        print("\nSample Results:")
        print(df[sample_columns].head())

        return df

# Usage
if __name__ == "__main__":
    # Instantiate and run optimizer
    optimizer = LogisticsRouteOptimizer()
    results_df = optimizer.run_optimization()

    # Optional: Display summary statistics
    total_routes = sum(len(routes) for routes in results_df['Viable_Middle_Airports'])
    print(f"\nSummary:")
    print(f"Total shipments processed: {len(results_df)}")
    print(f"Total viable routes found: {total_routes}")
    print(f"Average routes per shipment: {total_routes/len(results_df):.2f}")

"""## Correct Mapping"""

df.head()

import pandas as pd
import requests
import time

# Load coordinates data
df = pd.read_csv("/content/branch_coordinates_new.csv")  # Replace with your actual file path

branches_list = [
    "A02-BARODA BRANCH", "A44-ANAND BRANCH", "A49-HALOL BRANCH", "A51-DAHEJ SORTING OFFICE",
    "H17-WARANGAL SUPER FRANCHISEE", "H37-KARIMNAGAR SUPER FRANCHISEE", "H53-NIZAMABAD SUPER FRANCHISEE",
    "H96-SHAMSHABAD BRANCH", "H98-MANCHERIAL SUPER FRANCHISEE", "J02-JALANDHAR BRANCH",
    "J05-AMRITSAR BRANCH", "J11-PATHANKOT BRANCH", "J12-BADDI SUPER FRANCHISEE", "J17-PHAGWARA BRANCH",
    "J19-NANGAL BRANCH", "J20-BILASPUR SUPER FRANCHISEE", "N37-JAMMU BRANCH", "N38-SRI NAGAR BRANCH",
    "K08-HOWRAH BRANCH", "K119-PUNJABI MORE BRANCH", "K128-EAST KOLKATA KONA STORAGE CENTER APEX",
    "K136-ANDAL AIR APEX", "K14-DURGAPUR BRANCH", "K22-ASANSOL BRANCH", "K32-MECHEDA BRANCH",
    "K37-BAHARAMPORE BRANCH", "K40-KHARAGPUR BRANCH", "K45-BURDWAN BRANCH", "K63-BANKURA SUPER FRANCHISE",
    "K72-KONA BRANCH", "K81-SERAMPORE SUPER FRANCHISEE", "K89-HALDIA BRANCH",
    "L01-GURGAON DP PROCESSING CENTER APEX", "L02-GURGAON MASTER FRANCHISEE",
    "L03-GURGAON - HERO HONDA CHOWK BRANCH", "L04-GURGAON BRANCH", "L27-GURGAON - SOHNA ROAD BRANCH",
    "Q07-ALWAR BRANCH", "Q34-BAWAL BRANCH", "Q44-NEEMRANA BRANCH", "L05-DHARUHERA BRANCH",
    "L23-MANESAR BRANCH", "L28-NUH RURAL BRANCH", "L30-GURGAON T/S APEX", "Q11-BHIWADI BRANCH",
    "Q18-BEHROR SORTING OFFICE", "P19-NASIK BRANCH", "P61-JALNA SORTING OFFICE",
    "P74-YEOLA SUPER FRANCHISEE", "R07-BILASPUR BRANCH", "R08-BHILLAI BRANCH", "R10-KORBA BRANCH",
    "R19-RAIGARH BRANCH", "W07-SAMBALPUR BRANCH", "W23-BOLANGIR SORTING OFFICE", "S01-KHIRKI BRANCH",
    "S03-MAHIPALPUR BRANCH", "S04-OKHLA BRANCH", "S09-DWARKA BRANCH", "S13-OKHLA APEX",
    "S17-ASHRAM BRANCH", "S25-SOUTH DELHI DP PROCESSING CENTER APEX", "S26-CHATTARPUR BRANCH",
    "U21-SAHIBABAD BRANCH", "U30-VAISHALI BRANCH", "Q40-BHARATPUR BRANCH", "U07-GWALIOR BRANCH",
    "U09-AGRA BRANCH", "U16-ALIGARH BRANCH", "U25-MATHURA BRANCH", "U14-MORADABAD BRANCH",
    "U27-KASHIPUR BRANCH", "U31-BIJNORE BRANCH", "U10-DEHRADUN BRANCH", "U17-RISHIKESH BRANCH",
    "U18-HARDWAR BRANCH", "U20-SAHARANPUR BRANCH", "U22-ROORKEE BRANCH", "U28-SELAQUI BRANCH",
    "U12-HALDWANI BRANCH", "U15-BAREILLY BRANCH", "U23-RUDRAPUR BRANCH",
    "U77-SHAHJAHANPUR SUPER FRANCHISEE", "N30-NOIDA APEX", "N31-GREATER NOIDA BRANCH",
    "N32-NOIDA PH3 BRANCH", "N35-PARI CHOWK BRANCH", "N42-ECOTECH 12 NOIDA BRANCH",
    "S02-FARIDABAD BRANCH", "S27-BALLABHGARH BRANCH", "U145-BULANDSHAHR BRANCH",
    "I44-SATNA SORTING OFFICE", "U03-KANPUR BRANCH", "U110-ORAI NODAL POINT",
    "U141-KANNUAJ BRANCH", "U150-KANPUR CITY BRANCH", "U24-JHANSI BRANCH", "U83-AURAIYA NODAL POINT"
]

hubs_list = [
    "A01-AHMEDABAD APEX", "A04-SURAT APEX", "B10-BANGALORE YELAHANKA APEX", "C42-GUINDY APEX",
    "E01-COIMBATORE APEX", "E04-SALEM APEX", "H13-HYDERABAD SHAMSHABAD APEX", "I01-INDORE APEX",
    "J10-AMBALA APEX", "J29-PANCHKULA APEX", "K16-KOLKATA RAJARHAT APEX", "K66-SILIGURI-MATIGARA APEX",
    "M10-MUMBAI SAKINAKA APEX", "N05-DELHI SAMALKHA APEX", "O06-COCHIN APEX", "P01-PUNE APEX",
    "Q05-JAIPUR APEX", "R01-NAGPUR AIR APEX", "T01-PATNA APEX", "T02-RANCHI APEX",
    "U04-VARANASI APEX", "U11-GHAZIABAD APEX", "U43-LUCKNOW APEX", "V02-VISHAKAPATNAM APEX",
    "V06-VIJAYAWADA APEX", "W53-BHUBANESWAR AIR APEX", "X01-DHARAPUR APEX", "X03-AGARTALA APEX",
    "X21-DIBRUGARH APEX"
]
# Filter branches and hubs
branches_df = df[df["Branch/Hub Name"].isin(branches_list)].reset_index(drop=True)
hubs_df = df[df["Branch/Hub Name"].isin(hubs_list)].reset_index(drop=True)

# Setup your Google API Key
API_KEY = "AIzaSyAqA_e_6qesz41KvD0s85iUXE4a2N2iRoY"  # Replace with your real key

# Final results list
results = []

# Loop through each branch
for i, branch in branches_df.iterrows():
    origin = f"{branch['Latitude']},{branch['Longitude']}"
    branch_name = branch["Branch/Hub Name"]

    destinations = "|".join([f"{lat},{lon}" for lat, lon in zip(hubs_df["Latitude"], hubs_df["Longitude"])])

    url = "https://maps.googleapis.com/maps/api/distancematrix/json"
    params = {
        "origins": origin,
        "destinations": destinations,
        "key": API_KEY,
        "mode": "driving"
    }

    # API call
    response = requests.get(url, params=params)
    data = response.json()

    if data["status"] != "OK":
        print(f"Error for branch {branch_name}: {data.get('error_message', 'Unknown error')}")
        continue

    elements = data["rows"][0]["elements"]

    # Find index of minimum travel time
    durations = [e["duration"]["value"] if e["status"] == "OK" else float('inf') for e in elements]
    distances = [e["distance"]["value"] if e["status"] == "OK" else float('inf') for e in elements]

    min_idx = durations.index(min(durations))

    nearest_hub = hubs_df.iloc[min_idx]
    results.append({
        "Branch": branch_name,
        "Nearest Hub": nearest_hub["Branch/Hub Name"],
        "Travel Time (min)": round(durations[min_idx] / 60, 2),
        "Distance (km)": round(distances[min_idx] / 1000, 2)
    })

    time.sleep(1)  # To avoid hitting rate limits

# Output as DataFrame
result_df = pd.DataFrame(results)
print(result_df)

# Optional: Save to file
# result_df.to_csv("branch_to_nearest_hub_google_api.csv", index=False)

import pandas as pd
import requests
import time
import math

# Load coordinates data
df = pd.read_csv("/content/branch_coordinates_new.csv")  # Replace with your actual file path

branches_list = [
    "A02-BARODA BRANCH", "A44-ANAND BRANCH", "A49-HALOL BRANCH", "A51-DAHEJ SORTING OFFICE",
    "H17-WARANGAL SUPER FRANCHISEE", "H37-KARIMNAGAR SUPER FRANCHISEE", "H53-NIZAMABAD SUPER FRANCHISEE",
    "H96-SHAMSHABAD BRANCH", "H98-MANCHERIAL SUPER FRANCHISEE", "J02-JALANDHAR BRANCH",
    "J05-AMRITSAR BRANCH", "J11-PATHANKOT BRANCH", "J12-BADDI SUPER FRANCHISEE", "J17-PHAGWARA BRANCH",
    "J19-NANGAL BRANCH", "J20-BILASPUR SUPER FRANCHISEE", "N37-JAMMU BRANCH", "N38-SRI NAGAR BRANCH",
    "K08-HOWRAH BRANCH", "K119-PUNJABI MORE BRANCH", "K128-EAST KOLKATA KONA STORAGE CENTER APEX",
    "K136-ANDAL AIR APEX", "K14-DURGAPUR BRANCH", "K22-ASANSOL BRANCH", "K32-MECHEDA BRANCH",
    "K37-BAHARAMPORE BRANCH", "K40-KHARAGPUR BRANCH", "K45-BURDWAN BRANCH", "K63-BANKURA SUPER FRANCHISE",
    "K72-KONA BRANCH", "K81-SERAMPORE SUPER FRANCHISEE", "K89-HALDIA BRANCH",
    "L01-GURGAON DP PROCESSING CENTER APEX", "L02-GURGAON MASTER FRANCHISEE",
    "L03-GURGAON - HERO HONDA CHOWK BRANCH", "L04-GURGAON BRANCH", "L27-GURGAON - SOHNA ROAD BRANCH",
    "Q07-ALWAR BRANCH", "Q34-BAWAL BRANCH", "Q44-NEEMRANA BRANCH", "L05-DHARUHERA BRANCH",
    "L23-MANESAR BRANCH", "L28-NUH RURAL BRANCH", "L30-GURGAON T/S APEX", "Q11-BHIWADI BRANCH",
    "Q18-BEHROR SORTING OFFICE", "P19-NASIK BRANCH", "P61-JALNA SORTING OFFICE",
    "P74-YEOLA SUPER FRANCHISEE", "R07-BILASPUR BRANCH", "R08-BHILLAI BRANCH", "R10-KORBA BRANCH",
    "R19-RAIGARH BRANCH", "W07-SAMBALPUR BRANCH", "W23-BOLANGIR SORTING OFFICE", "S01-KHIRKI BRANCH",
    "S03-MAHIPALPUR BRANCH", "S04-OKHLA BRANCH", "S09-DWARKA BRANCH", "S13-OKHLA APEX",
    "S17-ASHRAM BRANCH", "S25-SOUTH DELHI DP PROCESSING CENTER APEX", "S26-CHATTARPUR BRANCH",
    "U21-SAHIBABAD BRANCH", "U30-VAISHALI BRANCH", "Q40-BHARATPUR BRANCH", "U07-GWALIOR BRANCH",
    "U09-AGRA BRANCH", "U16-ALIGARH BRANCH", "U25-MATHURA BRANCH", "U14-MORADABAD BRANCH",
    "U27-KASHIPUR BRANCH", "U31-BIJNORE BRANCH", "U10-DEHRADUN BRANCH", "U17-RISHIKESH BRANCH",
    "U18-HARDWAR BRANCH", "U20-SAHARANPUR BRANCH", "U22-ROORKEE BRANCH", "U28-SELAQUI BRANCH",
    "U12-HALDWANI BRANCH", "U15-BAREILLY BRANCH", "U23-RUDRAPUR BRANCH",
    "U77-SHAHJAHANPUR SUPER FRANCHISEE", "N30-NOIDA APEX", "N31-GREATER NOIDA BRANCH",
    "N32-NOIDA PH3 BRANCH", "N35-PARI CHOWK BRANCH", "N42-ECOTECH 12 NOIDA BRANCH",
    "S02-FARIDABAD BRANCH", "S27-BALLABHGARH BRANCH", "U145-BULANDSHAHR BRANCH",
    "I44-SATNA SORTING OFFICE", "U03-KANPUR BRANCH", "U110-ORAI NODAL POINT",
    "U141-KANNUAJ BRANCH", "U150-KANPUR CITY BRANCH", "U24-JHANSI BRANCH", "U83-AURAIYA NODAL POINT"
]

hubs_list = [
    "A01-AHMEDABAD APEX", "A04-SURAT APEX", "B10-BANGALORE YELAHANKA APEX", "C42-GUINDY APEX",
    "E01-COIMBATORE APEX", "E04-SALEM APEX", "H13-HYDERABAD SHAMSHABAD APEX", "I01-INDORE APEX",
    "J10-AMBALA APEX", "J29-PANCHKULA APEX", "K16-KOLKATA RAJARHAT APEX", "K66-SILIGURI-MATIGARA APEX",
    "M10-MUMBAI SAKINAKA APEX", "N05-DELHI SAMALKHA APEX", "O06-COCHIN APEX", "P01-PUNE APEX",
    "Q05-JAIPUR APEX", "R01-NAGPUR AIR APEX", "T01-PATNA APEX", "T02-RANCHI APEX",
    "U04-VARANASI APEX", "U11-GHAZIABAD APEX", "U43-LUCKNOW APEX", "V02-VISHAKAPATNAM APEX",
    "V06-VIJAYAWADA APEX", "W53-BHUBANESWAR AIR APEX", "X01-DHARAPUR APEX", "X03-AGARTALA APEX",
    "X21-DIBRUGARH APEX"
]

# Filter coordinates
branches_df = df[df["Branch/Hub Name"].isin(branches_list)].reset_index(drop=True)
hubs_df = df[df["Branch/Hub Name"].isin(hubs_list)].reset_index(drop=True)

# Google API key
API_KEY = "AIzaSyAqA_e_6qesz41KvD0s85iUXE4a2N2iRoY"

# Batching function
def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

# Final results
results = []

# Loop through each branch
for _, branch in branches_df.iterrows():
    origin = f"{branch['Latitude']},{branch['Longitude']}"
    branch_name = branch["Branch/Hub Name"]

    all_durations = []
    all_distances = []
    all_hub_names = []

    # Batch the hubs into groups of 25
    for hub_chunk in chunks(hubs_df, 25):
        destinations = "|".join([f"{lat},{lon}" for lat, lon in zip(hub_chunk["Latitude"], hub_chunk["Longitude"])])

        url = "https://maps.googleapis.com/maps/api/distancematrix/json"
        params = {
            "origins": origin,
            "destinations": destinations,
            "key": API_KEY,
            "mode": "driving"
        }

        response = requests.get(url, params=params)
        data = response.json()

        if data.get("status") != "OK":
            print(f"Error for branch {branch_name}: {data.get('error_message', 'Unknown error')}")
            continue

        row = data["rows"][0]["elements"]

        for i, element in enumerate(row):
            if element["status"] == "OK":
                duration = element["duration"]["value"]
                distance = element["distance"]["value"]
                hub_name = hub_chunk.iloc[i]["Branch/Hub Name"]

                all_durations.append(duration)
                all_distances.append(distance)
                all_hub_names.append(hub_name)

        time.sleep(1)  # be polite to API

    # Find the minimum duration hub
    if all_durations:
        min_idx = all_durations.index(min(all_durations))
        results.append({
            "Branch": branch_name,
            "Nearest Hub": all_hub_names[min_idx],
            "Travel Time (min)": round(all_durations[min_idx] / 60, 2),
            "Distance (km)": round(all_distances[min_idx] / 1000, 2)
        })
    else:
        print(f"No valid hub distances returned for branch {branch_name}")

# Convert to DataFrame
df_result = pd.DataFrame(results)
print(df_result)

# Optionally save
# df_result.to_csv("branch_to_nearest_hub_googleapi.csv", index=False)

df_result.head(20)

df_result.to_csv("branch_to_nearest_hub_googleapi.csv", index=False)

"""## Code Running"""

import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import itertools
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

class AirPathOptimizer:
    def __init__(self):
        """Initialize the Air Path Optimizer with required data structures."""
        self.final_dataset = None
        self.airport_to_airport_time = None
        self.airport_volume = None
        self.flight_connection_data = None
        self.branch_to_hub_time = None
        self.hub_to_airport_time = None
        self.hub_to_hub_matrix = None
        self.hub_type_mapping = None

        # Constants
        self.HUB_PROCESSING_TIME = 2.5  # hours per hub
        self.IN_COOLING_TIME = 2.0      # hours
        self.OUT_COOLING_TIME = 2.0     # hours
        self.VOLUME_THRESHOLD = 45.0    # kg minimum volume

    def load_data(self, file_paths: Dict[str, str]):
        """Load all required CSV files."""
        try:
            print("Loading datasets...")

            self.final_dataset = pd.read_csv(file_paths['final_dataset'])
            self.airport_to_airport_time = pd.read_csv(file_paths['airport_to_airport_time'])
            self.airport_volume = pd.read_csv(file_paths['airport_volume'])
            self.flight_connection_data = pd.read_csv(file_paths['flight_connection_data'])
            self.branch_to_hub_time = pd.read_csv(file_paths['branch_to_hub_time'])
            self.hub_to_airport_time = pd.read_csv(file_paths['hub_to_airport_time'])
            self.hub_to_hub_matrix = pd.read_csv(file_paths['hub_to_hub_matrix'])
            self.hub_type_mapping = pd.read_csv(file_paths['hub_type_mapping'])

            print("All datasets loaded successfully!")
            self._preprocess_data()

        except Exception as e:
            print(f"Error loading data: {str(e)}")
            raise

    def _safe_numeric_conversion(self, value, default=0.0):
        """Safely convert value to float, handling strings and NaN."""
        try:
            if pd.isna(value) or value == '' or value is None:
                return default
            return float(value)
        except (ValueError, TypeError):
            return default

    def _preprocess_data(self):
        """Preprocess data for faster lookups with robust data cleaning."""
        print("Preprocessing data for optimization...")

        # Clean and convert branch_to_hub_time data
        self.branch_to_hub_dict = {}
        for _, row in self.branch_to_hub_time.iterrows():
            branch = str(row.iloc[0]).strip()
            hub = str(row.iloc[1]).strip()
            time_val = self._safe_numeric_conversion(row.iloc[2] if len(row) > 2 else 0)
            self.branch_to_hub_dict[branch] = time_val

        # Hub to branch lookup (reverse)
        self.hub_to_branch_dict = {}
        for _, row in self.branch_to_hub_time.iterrows():
            branch = str(row.iloc[0]).strip()
            hub = str(row.iloc[1]).strip()
            time_val = self._safe_numeric_conversion(row.iloc[2] if len(row) > 2 else 0)
            self.hub_to_branch_dict[branch] = time_val

        # Airport to airport time lookup
        self.flight_time_dict = {}
        for _, row in self.airport_to_airport_time.iterrows():
            origin = str(row['Origin']).strip().upper()
            dest = str(row['Destination']).strip().upper()
            time_minutes = self._safe_numeric_conversion(row['Estimated_Flight_Time_Minutes'])
            key = f"{origin}_{dest}"
            self.flight_time_dict[key] = time_minutes / 60.0  # Convert to hours

        # Airport volume lookup
        self.airport_volume_dict = {}
        for _, row in self.airport_volume.iterrows():
            origin = str(row['Origin_Airport']).strip().upper()
            dest = str(row['Destination_Airport']).strip().upper()
            volume = self._safe_numeric_conversion(row['Volume_kg'])
            key = f"{origin}_{dest}"
            self.airport_volume_dict[key] = volume

        # Hub to airport time lookup - handle various column structures
        self.hub_to_airport_dict = {}
        for _, row in self.hub_to_airport_time.iterrows():
            try:
                if 'Hub' in self.hub_to_airport_time.columns and 'Airport' in self.hub_to_airport_time.columns:
                    hub = str(row['Hub']).strip()
                    airport = str(row['Airport']).strip().upper()
                    time_val = self._safe_numeric_conversion(row.iloc[2])  # Assuming time is in 3rd column
                else:
                    hub = str(row.iloc[0]).strip()
                    airport = str(row.iloc[1]).strip().upper()
                    time_val = self._safe_numeric_conversion(row.iloc[2])

                key = f"{hub}_{airport}"
                self.hub_to_airport_dict[key] = time_val
                # Also create reverse lookup for airport to hub
                key_reverse = f"{airport}_{hub}"
                self.hub_to_airport_dict[key_reverse] = time_val
            except Exception as e:
                continue

        # Hub to hub matrix lookup
        self.hub_to_hub_dict = {}
        for _, row in self.hub_to_hub_matrix.iterrows():
            origin_hub = str(row.iloc[0]).strip()
            dest_hub = str(row.iloc[1]).strip()
            time_val = self._safe_numeric_conversion(row.iloc[2])
            key = f"{origin_hub}_{dest_hub}"
            self.hub_to_hub_dict[key] = time_val

        # Hub type mapping
        self.hub_type_dict = {}
        for _, row in self.hub_type_mapping.iterrows():
            hub = str(row.iloc[0]).strip()
            hub_type = str(row.iloc[1]).strip() if len(row) > 1 else 'Air & Surface'
            self.hub_type_dict[hub] = hub_type

        # Create flight connection sets for faster lookup
        self.flight_connections = set()
        for _, row in self.flight_connection_data.iterrows():
            origin = str(row['Origin City']).strip().upper()
            dest = str(row['Destination City']).strip().upper()
            self.flight_connections.add(f"{origin}_{dest}")

        print("Data preprocessing completed!")

    def get_middle_airports(self) -> List[str]:
        """Get list of potential middle airports from flight connection data."""
        airports = set()
        airports.update([str(x).strip().upper() for x in self.flight_connection_data['Origin City'].unique()])
        airports.update([str(x).strip().upper() for x in self.flight_connection_data['Destination City'].unique()])
        return list(airports)

    def check_flight_availability(self, origin: str, destination: str) -> bool:
        """Check if flight connection exists between two airports."""
        origin = str(origin).strip().upper()
        destination = str(destination).strip().upper()
        return f"{origin}_{destination}" in self.flight_connections

    def get_flight_time(self, origin: str, destination: str) -> float:
        """Get flight time between two airports in hours."""
        origin = str(origin).strip().upper()
        destination = str(destination).strip().upper()
        key = f"{origin}_{destination}"
        return self.flight_time_dict.get(key, 0.0)

    def get_airport_volume(self, origin: str, destination: str) -> float:
        """Get existing volume between two airports."""
        origin = str(origin).strip().upper()
        destination = str(destination).strip().upper()
        key = f"{origin}_{destination}"
        return self.airport_volume_dict.get(key, 0.0)

    def get_hub_for_airport(self, airport: str) -> str:
        """Get or create hub name for an airport."""
        airport = str(airport).strip().upper()
        # Try to find existing hub mapping
        for key in self.hub_to_airport_dict.keys():
            if key.endswith(f"_{airport}"):
                return key.split('_')[0]

        # Create default hub name if not found
        return f"{airport}_HUB"

    def calculate_path_time(self, origin_branch: str, dest_branch: str,
                           middle_airport: str, od_volume: float) -> Dict:
        """Calculate total time for a path via middle airport."""
        result = {
            'valid': False,
            'path': '',
            'times': {},
            'total_air_time': 0,
            'total_road_time': 0,
            'volume_check': False,
            'details': {}
        }

        try:
            # Get origin and destination info from final_dataset
            od_matches = self.final_dataset[
                (self.final_dataset['Origin Branch'] == origin_branch) &
                (self.final_dataset['Destination Branch'] == dest_branch)
            ]

            if od_matches.empty:
                return result

            od_row = od_matches.iloc[0]

            origin_hub = str(od_row['Origin Hub']).strip()
            origin_airport = str(od_row['Origin Airport']).strip().upper()
            dest_airport = str(od_row['Destination Airport']).strip().upper()
            dest_hub = str(od_row['Destination Hub']).strip()
            middle_airport = str(middle_airport).strip().upper()
            od_volume = self._safe_numeric_conversion(od_volume)

            # Check if flights exist
            if not (self.check_flight_availability(origin_airport, middle_airport) and
                    self.check_flight_availability(middle_airport, dest_airport)):
                return result

            # Check volume threshold
            existing_volume = self.get_airport_volume(origin_airport, middle_airport)
            total_volume = existing_volume + od_volume

            if total_volume < self.VOLUME_THRESHOLD:
                result['volume_check'] = False
                return result

            result['volume_check'] = True

            # Calculate each segment time with safe lookups
            times = {}

            # T1: Origin Branch → Origin Hub
            times['T1'] = self.branch_to_hub_dict.get(origin_branch, 1.0)

            # T2: Origin Hub → Origin Airport
            key = f"{origin_hub}_{origin_airport}"
            times['T2'] = self.hub_to_airport_dict.get(key, 1.0)

            # T3: Origin Airport → Middle Airport (flight)
            times['T3'] = self.get_flight_time(origin_airport, middle_airport)
            if times['T3'] == 0:
                return result  # No flight time available

            # T4: Middle Airport → Middle Airport's Air Hub
            middle_air_hub = self.get_hub_for_airport(middle_airport)
            key = f"{middle_airport}_{middle_air_hub}"
            times['T4'] = self.hub_to_airport_dict.get(key, 1.0)

            # T5: Air Hub → Surface Hub (if different)
            times['T5'] = 0
            middle_surface_hub = middle_air_hub
            hub_type = self.hub_type_dict.get(middle_air_hub, 'Air & Surface')

            if 'Air' in hub_type and 'Surface' not in hub_type:
                # Need separate surface hub
                middle_surface_hub = f"{middle_airport}_SURFACE_HUB"
                key = f"{middle_air_hub}_{middle_surface_hub}"
                times['T5'] = self.hub_to_hub_dict.get(key, 1.0)

            # T6: Surface Hub → Destination Hub
            key = f"{middle_surface_hub}_{dest_hub}"
            times['T6'] = self.hub_to_hub_dict.get(key, 8.0)  # Default inter-city hub time

            # T7: Destination Hub → Destination Branch
            times['T7'] = self.hub_to_branch_dict.get(dest_branch, 1.0)

            # Calculate processing and cooling times
            num_hubs = 4 if times['T5'] > 0 else 3
            processing_time = num_hubs * self.HUB_PROCESSING_TIME
            cooling_time = self.IN_COOLING_TIME + self.OUT_COOLING_TIME

            # Total air path time
            total_air_time = sum(times.values()) + processing_time + cooling_time

            # Calculate road path time for comparison
            road_t1 = times['T1']
            road_hub_to_hub = self.hub_to_hub_dict.get(f"{origin_hub}_{dest_hub}", 12.0)
            road_t7 = times['T7']
            road_processing = 2 * self.HUB_PROCESSING_TIME

            total_road_time = road_t1 + road_hub_to_hub + road_t7 + road_processing

            # Build path string
            path_components = [
                origin_branch,
                origin_hub,
                f"{origin_airport} Airport",
                f"{middle_airport} Airport",
                middle_air_hub
            ]

            if times['T5'] > 0:
                path_components.append(middle_surface_hub)

            path_components.extend([dest_hub, dest_branch])

            result.update({
                'valid': True,
                'path': ' → '.join(path_components),
                'times': times,
                'total_air_time': total_air_time,
                'total_road_time': total_road_time,
                'processing_time': processing_time,
                'cooling_time': cooling_time,
                'existing_volume': existing_volume,
                'total_volume': total_volume,
                'details': {
                    'middle_airport': middle_airport,
                    'num_hubs': num_hubs
                }
            })

        except Exception as e:
            # Suppress individual errors to avoid spam
            pass

        return result

    def process_single_od_pair(self, row_data: Tuple) -> List[Dict]:
        """Process a single OD pair - for parallel processing."""
        idx, row = row_data
        results = []

        if row['Path_Required'] != 'YES':
            return results

        origin_branch = row['Origin Branch']
        dest_branch = row['Destination Branch']
        product = row['New Product GCP']
        od_volume = self._safe_numeric_conversion(row['Total Weight'])

        # Get middle airports (cached)
        middle_airports = self.middle_airports_list

        # Find all viable paths
        viable_paths = []

        for middle_airport in middle_airports:
            # Skip if middle airport is same as origin or destination
            if middle_airport in [str(row['Origin Airport']).strip().upper(),
                                str(row['Destination Airport']).strip().upper()]:
                continue

            path_result = self.calculate_path_time(
                origin_branch, dest_branch, middle_airport, od_volume
            )

            if path_result['valid'] and path_result['volume_check']:
                viable_paths.append({
                    'middle_airport': middle_airport,
                    'total_air_time': path_result['total_air_time'],
                    'total_road_time': path_result['total_road_time'],
                    'path': path_result['path'],
                    'times': path_result['times'],
                    'existing_volume': path_result['existing_volume'],
                    'total_volume': path_result['total_volume']
                })

        # Sort by total air time and take top 3
        viable_paths.sort(key=lambda x: x['total_air_time'])
        top_paths = viable_paths[:3]

        # Create results for each top path
        for i, path in enumerate(top_paths, 1):
            recommendation = "Air Route" if path['total_air_time'] < path['total_road_time'] else "Road Route"

            result = {
                'Origin Branch': origin_branch,
                'Destination Branch': dest_branch,
                'Product': product,
                'Route Rank': i,
                'Suggested Path': path['path'],
                'T1': round(path['times']['T1'], 2),
                'T2': round(path['times']['T2'], 2),
                'T3': round(path['times']['T3'], 2),
                'T4': round(path['times']['T4'], 2),
                'T5': round(path['times']['T5'], 2),
                'T6': round(path['times']['T6'], 2),
                'T7': round(path['times']['T7'], 2),
                'Total_Air_Time': round(path['total_air_time'], 2),
                'Total_Road_Time': round(path['total_road_time'], 2),
                'Volume_OD': od_volume,
                'Volume_to_Middle': path['existing_volume'],
                'Total_Volume': path['total_volume'],
                'Recommendation': recommendation,
                'Time_Savings': round(path['total_road_time'] - path['total_air_time'], 2)
            }

            results.append(result)

        return results

    def find_best_routes(self, max_routes: int = 3, use_parallel: bool = True, max_workers: int = 4, batch_size: int = 1000) -> List[Dict]:
        """Find best alternate routes for all OD pairs requiring routing."""
        # Filter dataset for only required paths
        required_paths = self.final_dataset[self.final_dataset['Path_Required'] == 'YES'].copy()

        print(f"Processing {len(required_paths)} OD pairs that require routing...")

        # Cache middle airports list
        self.middle_airports_list = self.get_middle_airports()
        print(f"Available middle airports: {len(self.middle_airports_list)}")

        all_results = []
        start_time = time.time()

        if use_parallel and len(required_paths) > 100:
            print(f"Using parallel processing with {max_workers} workers...")

            # Process in batches to manage memory
            for batch_start in range(0, len(required_paths), batch_size):
                batch_end = min(batch_start + batch_size, len(required_paths))
                batch_data = required_paths.iloc[batch_start:batch_end]
                print(f"Processing batch {batch_start//batch_size + 1}/{(len(required_paths)-1)//batch_size + 1} ({len(batch_data)} records)")

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # Submit all tasks
                    futures = [executor.submit(self.process_single_od_pair, (idx, row))
                              for idx, row in batch_data.iterrows()]

                    # Collect results with progress tracking
                    batch_results = []
                    for i, future in enumerate(as_completed(futures)):
                        try:
                            result = future.result()
                            batch_results.extend(result)
                            if (i + 1) % 50 == 0:
                                print(f"  Completed {i + 1}/{len(futures)} records in batch")
                        except Exception as e:
                            print(f"  Error processing record: {str(e)}")
                            continue

                all_results.extend(batch_results)
                print(f"Batch completed. Found {len(batch_results)} viable routes so far: {len(all_results)} total")

        else:
            print("Using sequential processing...")
            for idx, row in required_paths.iterrows():
                if idx % 100 == 0:
                    elapsed = time.time() - start_time
                    rate = idx / elapsed if elapsed > 0 else 0
                    eta = (len(required_paths) - idx) / rate if rate > 0 else 0
                    print(f"Processed {idx}/{len(required_paths)} records ({rate:.1f} rec/sec, ETA: {eta/60:.1f} min)")

                results = self.process_single_od_pair((idx, row))
                all_results.extend(results)

        elapsed = time.time() - start_time
        print(f"\nProcessing completed in {elapsed/60:.1f} minutes")
        print(f"Total viable routes found: {len(all_results)}")

        return all_results

    def generate_output(self, results: List[Dict], output_file: str = 'air_path_optimization_results.csv'):
        """Generate final output CSV file."""
        if not results:
            print("No viable routes found!")
            return pd.DataFrame()

        df_results = pd.DataFrame(results)
        df_results.to_csv(output_file, index=False)

        print(f"\n=== AIR PATH OPTIMIZATION RESULTS ===")
        print(f"Total OD pairs with viable routes: {len(df_results['Origin Branch'].unique())}")
        print(f"Total viable routes found: {len(df_results)}")
        print(f"Results saved to: {output_file}")

        # Summary statistics
        air_recommended = len(df_results[df_results['Recommendation'] == 'Air Route'])
        road_recommended = len(df_results[df_results['Recommendation'] == 'Road Route'])

        print(f"\nRecommendation Summary:")
        print(f"Air Route recommended: {air_recommended} routes ({air_recommended/len(df_results)*100:.1f}%)")
        print(f"Road Route recommended: {road_recommended} routes ({road_recommended/len(df_results)*100:.1f}%)")

        if len(df_results) > 0:
            avg_air_time = df_results['Total_Air_Time'].mean()
            avg_road_time = df_results['Total_Road_Time'].mean()
            avg_savings = df_results[df_results['Time_Savings'] > 0]['Time_Savings'].mean()

            print(f"\nAverage Times:")
            print(f"Air path time: {avg_air_time:.2f} hours")
            print(f"Road path time: {avg_road_time:.2f} hours")
            if not pd.isna(avg_savings):
                print(f"Average time savings (when air is better): {avg_savings:.2f} hours")

        return df_results

def main():
    """Main function to run the air path optimization."""

    # Define file paths - UPDATE THESE PATHS ACCORDING TO YOUR FILE LOCATIONS
    file_paths = {
        'final_dataset': '/content/Daily_datafor_(EP+BP)_ES_path_required_updated.csv',
        'airport_to_airport_time': '/content/acutal_flight_time_matrix.csv',
        'airport_volume': '/content/Total_volume_from_airport_to_air(sample).csv',
        'flight_connection_data': '/content/flight_connection(sample).csv',
        'branch_to_hub_time': '/content/branch_to_hub_time_haversine.csv',
        'hub_to_airport_time': '/content/Hub_to_airport_time_matrix - Sheet1.csv',
        'hub_to_hub_matrix': '/content/hub_to_hub_time.csv',
        'hub_type_mapping': '/content/Hub_(Surface & Air).csv'
    }
    try:
        # Initialize optimizer
        optimizer = AirPathOptimizer()

        # Load data
        optimizer.load_data(file_paths)

        # Find best routes with optimizations
        print("\nFinding optimal routes...")
        results = optimizer.find_best_routes(
            max_routes=3,
            use_parallel=True,  # Set to False if you want sequential processing
            max_workers=4,      # Adjust based on your system
            batch_size=1000     # Process in smaller batches
        )

        # Generate output
        df_results = optimizer.generate_output(results)

        # Display sample results
        if len(results) > 0:
            print(f"\nSample Results (first 5 routes):")
            sample_cols = ['Origin Branch', 'Destination Branch', 'Route Rank', 'Total_Air_Time', 'Total_Road_Time', 'Recommendation']
            if not df_results.empty:
                print(df_results[sample_cols].head().to_string(index=False))

        return df_results

    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        raise

if __name__ == "__main__":
    results_df = main()

import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import itertools

class AirPathOptimizer:
    def __init__(self):
        """Initialize the Air Path Optimizer with required data structures."""
        self.final_dataset = None
        self.airport_to_airport_time = None
        self.airport_volume = None
        self.flight_connection_data = None
        self.branch_to_hub_time = None
        self.hub_to_airport_time = None
        self.hub_to_hub_matrix = None
        self.hub_type_mapping = None

        # Constants
        self.HUB_PROCESSING_TIME = 2.5  # hours per hub
        self.IN_COOLING_TIME = 2.0      # hours
        self.OUT_COOLING_TIME = 2.0     # hours
        self.VOLUME_THRESHOLD = 45.0    # kg minimum volume

    def load_data(self, file_paths: Dict[str, str]):
        """Load all required CSV files."""
        try:
            print("Loading datasets...")

            self.final_dataset = pd.read_csv(file_paths['final_dataset'])
            self.airport_to_airport_time = pd.read_csv(file_paths['airport_to_airport_time'])
            self.airport_volume = pd.read_csv(file_paths['airport_volume'])
            self.flight_connection_data = pd.read_csv(file_paths['flight_connection_data'])
            self.branch_to_hub_time = pd.read_csv(file_paths['branch_to_hub_time'])
            self.hub_to_airport_time = pd.read_csv(file_paths['hub_to_airport_time'])
            self.hub_to_hub_matrix = pd.read_csv(file_paths['hub_to_hub_matrix'])
            self.hub_type_mapping = pd.read_csv(file_paths['hub_type_mapping'])

            print("All datasets loaded successfully!")
            self._preprocess_data()

        except Exception as e:
            print(f"Error loading data: {str(e)}")
            raise

    def _preprocess_data(self):
        """Preprocess data for faster lookups."""
        # Create lookup dictionaries for faster access
        self.branch_to_hub_dict = dict(zip(self.branch_to_hub_time.iloc[:, 0],
                                          self.branch_to_hub_time.iloc[:, 1]))

        # Hub to branch lookup (reverse)
        self.hub_to_branch_dict = dict(zip(self.branch_to_hub_time.iloc[:, 1],
                                          self.branch_to_hub_time.iloc[:, 0]))

        # Airport to airport time lookup
        self.flight_time_dict = {}
        for _, row in self.airport_to_airport_time.iterrows():
            key = f"{row['Origin']}_{row['Destination']}"
            self.flight_time_dict[key] = row['Estimated_Flight_Time_Minutes'] / 60.0  # Convert to hours

        # Airport volume lookup
        self.airport_volume_dict = {}
        for _, row in self.airport_volume.iterrows():
            key = f"{row['Origin_Airport']}_{row['Destination_Airport']}"
            self.airport_volume_dict[key] = row['Volume_kg']

        # Hub to airport time lookup
        self.hub_to_airport_dict = {}
        for _, row in self.hub_to_airport_time.iterrows():
            # Assuming columns are Hub, Airport, Time
            key = f"{row.iloc[0]}_{row.iloc[1]}"
            self.hub_to_airport_dict[key] = row.iloc[2]

        # Hub to hub matrix lookup
        self.hub_to_hub_dict = {}
        for _, row in self.hub_to_hub_matrix.iterrows():
            key = f"{row.iloc[0]}_{row.iloc[1]}"
            self.hub_to_hub_dict[key] = row.iloc[2]

        # Hub type mapping
        self.hub_type_dict = dict(zip(self.hub_type_mapping.iloc[:, 0],
                                     self.hub_type_mapping.iloc[:, 1]))

    def get_middle_airports(self) -> List[str]:
        """Get list of potential middle airports from flight connection data."""
        airports = set()
        airports.update(self.flight_connection_data['Origin City'].unique())
        airports.update(self.flight_connection_data['Destination City'].unique())
        return list(airports)

    def check_flight_availability(self, origin: str, destination: str, flight_type: str = None) -> bool:
        """Check if flight connection exists between two airports."""
        matches = self.flight_connection_data[
            (self.flight_connection_data['Origin City'] == origin) &
            (self.flight_connection_data['Destination City'] == destination)
        ]

        if flight_type:
            matches = matches[matches['Flight Type'] == flight_type]

        return len(matches) > 0

    def get_flight_time(self, origin: str, destination: str) -> Optional[float]:
        """Get flight time between two airports in hours."""
        key = f"{origin}_{destination}"
        return self.flight_time_dict.get(key)

    def get_airport_volume(self, origin: str, destination: str) -> float:
        """Get existing volume between two airports."""
        key = f"{origin}_{destination}"
        return self.airport_volume_dict.get(key, 0.0)

    def get_hub_from_airport(self, airport: str) -> Optional[str]:
        """Get hub associated with an airport."""
        # This assumes there's a mapping from airport to hub in your data
        # You might need to adjust this based on your actual data structure
        for hub, mapped_airport in self.hub_to_airport_dict.items():
            if hub.endswith(f"_{airport}"):
                return hub.split('_')[0]
        return None

    def calculate_path_time(self, origin_branch: str, dest_branch: str,
                           middle_airport: str, od_volume: float) -> Dict:
        """Calculate total time for a path via middle airport."""
        result = {
            'valid': False,
            'path': '',
            'times': {},
            'total_air_time': 0,
            'total_road_time': 0,
            'volume_check': False,
            'details': {}
        }

        try:
            # Get origin and destination info from final_dataset
            od_row = self.final_dataset[
                (self.final_dataset['Origin Branch'] == origin_branch) &
                (self.final_dataset['Destination Branch'] == dest_branch)
            ].iloc[0]

            origin_hub = od_row['Origin Hub']
            origin_airport = od_row['Origin Airport']
            dest_airport = od_row['Destination Airport']
            dest_hub = od_row['Destination Hub']

            # Check if flights exist
            if not (self.check_flight_availability(origin_airport, middle_airport) and
                    self.check_flight_availability(middle_airport, dest_airport)):
                return result

            # Check volume threshold
            existing_volume = self.get_airport_volume(origin_airport, middle_airport)
            total_volume = existing_volume + od_volume

            if total_volume < self.VOLUME_THRESHOLD:
                result['volume_check'] = False
                return result

            result['volume_check'] = True

            # Calculate each segment time
            times = {}

            # T1: Origin Branch → Origin Hub
            times['T1'] = self.branch_to_hub_dict.get(origin_branch, 0)

            # T2: Origin Hub → Origin Airport
            key = f"{origin_hub}_{origin_airport}"
            times['T2'] = self.hub_to_airport_dict.get(key, 0)

            # T3: Origin Airport → Middle Airport (flight)
            times['T3'] = self.get_flight_time(origin_airport, middle_airport) or 0

            # T4: Middle Airport → Middle Airport's Air Hub
            middle_air_hub = self.get_hub_from_airport(middle_airport) or f"{middle_airport}_AIR_HUB"
            key = f"{middle_air_hub}_{middle_airport}"
            times['T4'] = self.hub_to_airport_dict.get(key, 0)

            # T5: Air Hub → Surface Hub (if different)
            times['T5'] = 0
            middle_surface_hub = middle_air_hub
            hub_type = self.hub_type_dict.get(middle_air_hub, 'Air & Surface')

            if 'Air' in hub_type and 'Surface' not in hub_type:
                # Need separate surface hub
                middle_surface_hub = f"{middle_airport}_SURFACE_HUB"
                key = f"{middle_air_hub}_{middle_surface_hub}"
                times['T5'] = self.hub_to_hub_dict.get(key, 1.0)  # Default 1 hour if not found

            # T6: Surface Hub → Destination Hub
            key = f"{middle_surface_hub}_{dest_hub}"
            times['T6'] = self.hub_to_hub_dict.get(key, 0)

            # T7: Destination Hub → Destination Branch
            times['T7'] = self.hub_to_branch_dict.get(dest_branch, 0)

            # Calculate processing and cooling times
            num_hubs = 4 if times['T5'] > 0 else 3  # Origin, Middle Air, Middle Surface (if separate), Destination
            processing_time = num_hubs * self.HUB_PROCESSING_TIME
            cooling_time = self.IN_COOLING_TIME + self.OUT_COOLING_TIME

            # Total air path time
            total_air_time = sum(times.values()) + processing_time + cooling_time

            # Calculate road path time for comparison
            road_t1 = times['T1']  # Same as air path
            road_hub_to_hub = self.hub_to_hub_dict.get(f"{origin_hub}_{dest_hub}", 0)
            road_t7 = times['T7']  # Same as air path
            road_processing = 2 * self.HUB_PROCESSING_TIME  # Only 2 hubs for road

            total_road_time = road_t1 + road_hub_to_hub + road_t7 + road_processing

            # Build path string
            path_components = [
                origin_branch,
                origin_hub,
                f"{origin_airport} Airport",
                f"{middle_airport} Airport",
                middle_air_hub
            ]

            if times['T5'] > 0:
                path_components.append(middle_surface_hub)

            path_components.extend([dest_hub, dest_branch])

            result.update({
                'valid': True,
                'path': ' → '.join(path_components),
                'times': times,
                'total_air_time': total_air_time,
                'total_road_time': total_road_time,
                'processing_time': processing_time,
                'cooling_time': cooling_time,
                'existing_volume': existing_volume,
                'total_volume': total_volume,
                'details': {
                    'middle_airport': middle_airport,
                    'num_hubs': num_hubs
                }
            })

        except Exception as e:
            print(f"Error calculating path for {origin_branch} → {dest_branch} via {middle_airport}: {str(e)}")

        return result

    def find_best_routes(self, max_routes: int = 3) -> List[Dict]:
        """Find best alternate routes for all OD pairs requiring routing."""
        results = []
        middle_airports = self.get_middle_airports()

        print(f"Processing {len(self.final_dataset)} OD pairs...")
        print(f"Available middle airports: {len(middle_airports)}")

        for idx, row in self.final_dataset.iterrows():
            if row['Path_Required'] != 'YES':
                continue

            origin_branch = row['Origin Branch']
            dest_branch = row['Destination Branch']
            product = row['New Product GCP']
            od_volume = row['Total Weight']

            print(f"Processing: {origin_branch} → {dest_branch}")

            # Find all viable paths
            viable_paths = []

            for middle_airport in middle_airports:
                # Skip if middle airport is same as origin or destination
                if middle_airport in [row['Origin Airport'], row['Destination Airport']]:
                    continue

                path_result = self.calculate_path_time(
                    origin_branch, dest_branch, middle_airport, od_volume
                )

                if path_result['valid'] and path_result['volume_check']:
                    viable_paths.append({
                        'middle_airport': middle_airport,
                        'total_air_time': path_result['total_air_time'],
                        'total_road_time': path_result['total_road_time'],
                        'path': path_result['path'],
                        'times': path_result['times'],
                        'existing_volume': path_result['existing_volume'],
                        'total_volume': path_result['total_volume']
                    })

            # Sort by total air time and take top 3
            viable_paths.sort(key=lambda x: x['total_air_time'])
            top_paths = viable_paths[:max_routes]

            # Create results for each top path
            for i, path in enumerate(top_paths, 1):
                recommendation = "Air Route" if path['total_air_time'] < path['total_road_time'] else "Road Route"

                result = {
                    'Origin Branch': origin_branch,
                    'Destination Branch': dest_branch,
                    'Product': product,
                    'Route Rank': i,
                    'Suggested Path': path['path'],
                    'T1': path['times']['T1'],
                    'T2': path['times']['T2'],
                    'T3': path['times']['T3'],
                    'T4': path['times']['T4'],
                    'T5': path['times']['T5'],
                    'T6': path['times']['T6'],
                    'T7': path['times']['T7'],
                    'Total_Air_Time': round(path['total_air_time'], 2),
                    'Total_Road_Time': round(path['total_road_time'], 2),
                    'Volume_OD': od_volume,
                    'Volume_to_Middle': path['existing_volume'],
                    'Total_Volume': path['total_volume'],
                    'Recommendation': recommendation,
                    'Time_Savings': round(path['total_road_time'] - path['total_air_time'], 2)
                }

                results.append(result)

        return results

    def generate_output(self, results: List[Dict], output_file: str = 'air_path_optimization_results.csv'):
        """Generate final output CSV file."""
        if not results:
            print("No viable routes found!")
            return

        df_results = pd.DataFrame(results)
        df_results.to_csv(output_file, index=False)

        print(f"\n=== AIR PATH OPTIMIZATION RESULTS ===")
        print(f"Total OD pairs processed: {len(df_results['Origin Branch'].unique())}")
        print(f"Total viable routes found: {len(df_results)}")
        print(f"Results saved to: {output_file}")

        # Summary statistics
        air_recommended = len(df_results[df_results['Recommendation'] == 'Air Route'])
        road_recommended = len(df_results[df_results['Recommendation'] == 'Road Route'])

        print(f"\nRecommendation Summary:")
        print(f"Air Route recommended: {air_recommended} routes")
        print(f"Road Route recommended: {road_recommended} routes")

        if len(df_results) > 0:
            avg_air_time = df_results['Total_Air_Time'].mean()
            avg_road_time = df_results['Total_Road_Time'].mean()
            avg_savings = df_results['Time_Savings'].mean()

            print(f"\nAverage Times:")
            print(f"Air path time: {avg_air_time:.2f} hours")
            print(f"Road path time: {avg_road_time:.2f} hours")
            print(f"Average time savings (Air vs Road): {avg_savings:.2f} hours")

        return df_results

def main():
    """Main function to run the air path optimization."""

    # Define file paths - UPDATE THESE PATHS ACCORDING TO YOUR FILE LOCATIONS
    file_paths = {
        'final_dataset': '/content/Daily_datafor_(EP+BP)_ES_path_required_updated.csv',
        'airport_to_airport_time': '/content/acutal_flight_time_matrix.csv',
        'airport_volume': '/content/Total_volume_from_airport_to_air(sample).csv',
        'flight_connection_data': '/content/flight_connection(sample).csv',
        'branch_to_hub_time': '/content/branch_to_hub_time_haversine.csv',
        'hub_to_airport_time': '/content/Hub_to_airport_time_matrix - Sheet1.csv',
        'hub_to_hub_matrix': '/content/hub_to_hub_time.csv',
        'hub_type_mapping': '/content/Hub_(Surface & Air).csv'
    }

    try:
        # Initialize optimizer
        optimizer = AirPathOptimizer()

        # Load data
        optimizer.load_data(file_paths)

        # Find best routes
        print("\nFinding optimal routes...")
        results = optimizer.find_best_routes(max_routes=3)

        # Generate output
        df_results = optimizer.generate_output(results)

        # Display sample results
        if len(results) > 0:
            print(f"\nSample Results (first 5 routes):")
            print(df_results.head().to_string(index=False))

        return df_results

    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        raise

if __name__ == "__main__":
    results_df = main()

